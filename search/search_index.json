{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ocean4DVarNet documentation!","text":""},{"location":"#about-ocean4dvarnet","title":"About ocean4DVarNet","text":"<p>4DVarNet (short for 4D-Variational Network) is a deep learning-based framework that combines ideas from data assimilation (specifically 4D-Var) with neural networks to reconstruct high-resolution spatiotemporal data from incomplete and noisy observations\u2014commonly applied in Earth sciences, like oceanography and meteorology.</p>"},{"location":"#installing","title":"Installing","text":"<p>To install the package, you can use the following command: <pre><code>pip install ocean4dvarnet\n</code></pre></p> <p>Get more information in the installing section.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>import ocean4dvarnet\n</code></pre>"},{"location":"#ocean4dvarnet-extension","title":"Ocean4DVarnet Extension","text":"<p>This package contains the model's main functions. </p> <p>Another package is available, containing extended functionality : models, dataloader, etc. ... : Ocean4DVarnet-contrib - https://github.com/CIA-Oceanix/ocean4dvarnet-contrib</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to add specific models, data loaders, utilities, etc., the best way to do so is to contribute to the Ocean4DVarnet-contrib project.</p> <p>If you want to contribute to the core Ocean4DVarnet Package, please follow the information  in the contributing section.</p>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li>Project getting started : https://github.com/CIA-Oceanix/4dvarnet-starter</li> <li>4DVarNet papers:<ul> <li>Fablet, R.; Amar, M. M.; Febvre, Q.; Beauchamp, M.; Chapron, B. END-TO-END PHYSICS-INFORMED REPRESENTATION LEARNING FOR SA\u2121LITE OCEAN REMOTE SENSING DATA: APPLICATIONS TO SA\u2121LITE ALTIMETRY AND SEA SURFACE CURRENTS. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences 2021, V-3\u20132021, 295\u2013302. https://doi.org/10.5194/isprs-annals-v-3-2021-295-2021.</li> <li>Fablet, R.; Chapron, B.; Drumetz, L.; Mmin, E.; Pannekoucke, O.; Rousseau, F. Learning Variational Data Assimilation Models and Solvers. Journal of Advances in Modeling Earth Systems n/a (n/a), e2021MS002572. https://doi.org/10.1029/2021MS002572.</li> <li>Fablet, R.; Beauchamp, M.; Drumetz, L.; Rousseau, F. Joint Interpolation and Representation Learning for Irregularly Sampled Satellite-Derived Geophysical Fields. Frontiers in Applied Mathematics and Statistics 2021, 7. https://doi.org/10.3389/fams.2021.655224.</li> </ul> </li> </ul>"},{"location":"#license","title":"License","text":"<p>Copyright IMT Atlantique/OceaniX, contributor(s) : M. Beauchamp, R. Fablet, Q. Febvre, D. Zhu (IMT Atlantique)</p> <p>Contact person: ronan.fablet@imt-atlantique.fr.</p> <p>This software is a computer program whose purpose is to apply deep learning schemes to dynamical systems and ocean remote sensing data.</p> <p>This software is governed by the CeCILL-C license under French law and abiding by the rules of distribution of free software.</p> <p>You can use, modify and/ or redistribute the software under the terms of the CeCILL-C license as circulated by CEA, CNRS and INRIA at the following URL \"http://www.cecill.info\". As a counterpart to the access to the source code and rights to copy, modify and redistribute granted by the license, users are provided only with a limited warranty and the software's author, the holder of the economic rights, and the successive licensors have only limited liability.</p> <p>In this respect, the user's attention is drawn to the risks associated with loading, using, modifying and/or developing or reproducing the software by the user in light of its specific status of free software, that may mean that it is complicated to manipulate, and that also therefore means that it is reserved for developers and experienced professionals having in-depth computer knowledge. Users are therefore encouraged to load and test the software's suitability as regards their requirements in conditions enabling the security of their systems and/or data to be ensured and, more generally, to use and operate it in the same conditions as regards security. The fact that you are presently reading this means that you have had knowledge of the CeCILL-C license and that you accept its terms.</p>"},{"location":"concepts/","title":"Ocean4DVarNet","text":""},{"location":"concepts/#core-concept","title":"Core Concept","text":"<p>4DVarNet blends: - 4D-Variational data assimilation (4D-Var): A method used to optimally combine a numerical model with observations over a time window. - Deep learning: Neural networks learn to model complex dynamics and correct for model errors.</p> <p>It learns to reconstruct missing data over both space and time (the \"4D\" stands for 3D space + time).</p>"},{"location":"concepts/#how-it-works","title":"How It Works","text":"<ul> <li>It uses a neural network within a variational optimization loop.</li> <li>Given partial observations (like satellite data), it:<ol> <li>Predicts the full state over time.</li> <li>Minimizes a loss function that balances between:<ul> <li>Staying close to the observations.</li> <li>Being consistent with a learned dynamical prior (the network).</li> </ul> </li> <li>Refines its predictions iteratively (like traditional 4D-Var).</li> </ol> </li> </ul>"},{"location":"concepts/#applications","title":"Applications","text":"<ul> <li>Oceanographic reconstructions (e.g. Sea Surface Height fields).</li> <li>Climate modeling.</li> <li>Atmospheric reanalysis.</li> <li>Any setting where data is sparse, noisy, and spatiotemporal.</li> </ul>"},{"location":"concepts/#informations","title":"Informations","text":"<ul> <li>Project getting started : https://github.com/CIA-Oceanix/4dvarnet-starter</li> <li>4DVarNet papers:<ul> <li>Fablet, R.; Amar, M. M.; Febvre, Q.; Beauchamp, M.; Chapron, B. END-TO-END PHYSICS-INFORMED REPRESENTATION LEARNING FOR SA\u2121LITE OCEAN REMOTE SENSING DATA: APPLICATIONS TO SA\u2121LITE ALTIMETRY AND SEA SURFACE CURRENTS. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences 2021, V-3\u20132021, 295\u2013302. https://doi.org/10.5194/isprs-annals-v-3-2021-295-2021.</li> <li>Fablet, R.; Chapron, B.; Drumetz, L.; Mmin, E.; Pannekoucke, O.; Rousseau, F. Learning Variational Data Assimilation Models and Solvers. Journal of Advances in Modeling Earth Systems n/a (n/a), e2021MS002572. https://doi.org/10.1029/2021MS002572.</li> <li>Fablet, R.; Beauchamp, M.; Drumetz, L.; Rousseau, F. Joint Interpolation and Representation Learning for Irregularly Sampled Satellite-Derived Geophysical Fields. Frontiers in Applied Mathematics and Statistics 2021, 7. https://doi.org/10.3389/fams.2021.655224.</li> </ul> </li> </ul>"},{"location":"installing/","title":"Installing","text":"<p>Python Version: </p> <ul> <li>Python (&gt; 3.9)</li> </ul> <p>We require at least Python 3.9.</p>"},{"location":"installing/#environment-installation","title":"Environment Installation","text":"<p>We currently do not provide a conda build package of Ocean4DVarNet but only a pypi</p> <p>Ocean4DVarNet is based on CUDA/Pytorch. It can be quiet complicated to install this environement from pypi package.</p> <p>So the suggested installation is through mamba/conda for the CUDA/Pytorch environment, and  then pip form other dependencies.</p> <p>For Linux the process to make and use a mamba/conda environment is as follows : <pre><code>mamba create --name \"ocean-code\" python=3.12 -y\nmamba activate ocean-code\n</code></pre> Then the requirements must be installed in the environment : <pre><code>mamba env update -f environment.yaml\n</code></pre></p> <p>More informations about how to deploy a full environment can be found in the Development Set-up section</p>"},{"location":"installing/#instructions","title":"Instructions","text":"<p>To install the package, you can use the following command: <pre><code>pip install ocean4dvarnet\n</code></pre> or <pre><code>python -m pip install ocean4dvarnet\n</code></pre></p> <p>You can also get a zip or tgz archive of the package from github repository https://github.com/CIA-Oceanix/ocean4dvarnet/releases <pre><code>pip install ocean4dvarnet-contrib-x.x.x.tar.gz\n</code></pre></p> <p>And you can use the last development version directly from github repository <pre><code>pip install git+https://github.com/CIA-Oceanix/ocean4dvarnet.git\n</code></pre></p>"},{"location":"issues/","title":"Raise an issue","text":"<p>If you encounter a bug or have a feature request, the first step is to let us know by raising an issue on GitHub using the following steps:</p> <ol> <li>Check the existing issues to avoid duplicates.</li> <li>If it\u2019s a new issue, create a detailed bug report or feature request by filling in the issue template.</li> <li>Use clear, descriptive titles and provide as much relevant information as possible.</li> <li>If you have a bug, include the steps to reproduce it.</li> <li>If you have a feature request, describe the use case and expected behaviour.</li> <li>If you are interested in solving the issue yourself, assign the issue to yourself and follow the steps below.</li> </ol>"},{"location":"developing/","title":"Developing","text":"<p>bla bla bla</p>"},{"location":"developing/basic_env/","title":"Installation","text":"<ul> <li>Package that need build with wheel : with mamba (conda)<ul> <li>CUDA + Pytorch </li> </ul> </li> <li>Other packages : pip + requirement in pyproject.toml</li> </ul> <p>Pour une documentation d'installation plus d\u00e9taill\u00e9e</p>"},{"location":"developing/basic_env/#creation-de-lenvironnement-mamba","title":"Cr\u00e9ation de l'environnement mamba","text":"<ul> <li>Cr\u00e9er l'environnement : Attention, choisir la version de python qui correspond <pre><code>mamba create --name \"ocean-code\" python=3.12 -y\n</code></pre></li> <li> <p>Installer pip <pre><code>mamba install pip\n</code></pre></p> </li> <li> <p>Activer l'environnement <pre><code>mamba activate ocean-code\n</code></pre></p> </li> <li>afficher le contenu du fichier environment.yaml <pre><code>cat environment.yaml\n</code></pre></li> <li>Et lancer la mise \u00e0 jour de l'environnement (prend du temps) <pre><code>mamba env update -f environment.yaml\n</code></pre></li> </ul>"},{"location":"developing/ci-cd/","title":"Impl\u00e9m\u00e9ntation de Github Flow","text":""},{"location":"developing/ci-cd/#workflows","title":"Workflows","text":"<ul> <li>release : publish a code release</li> <li>create a relesae on Github</li> <li>publish a releas on pypi.org</li> <li>update-doc : update documentation</li> </ul>"},{"location":"developing/ci-cd/#secrets-a-configurer-dans-github","title":"Secrets \u00e0 configurer dans GitHub","text":"<p>Dans Settings &gt; Secrets and Variables &gt; Actions : Repository secrets</p> <ul> <li>PYPI_API_TOKEN : le token PyPI ou TestPyPI (selon le contexte)</li> </ul>"},{"location":"developing/contributing/","title":"Contributing","text":"<p>Thank you for your interest in Ocean4DVarNet! This guide will show you how to contribute to the Ocean4DVarNet package.</p> <p>These include general guidelines for contributions to Ocean4DVarNet, instructions on setting up a development environment, and guidelines on collaboration on GitHub, writing documentation, testing, and code style.</p> <p>Unit tests</p> <p>Ocean4DVarNet include unit tests that can be executed locally using pytest. For more information on testing, please refer to the general Ocean4DVarNet testing guidelines.</p>"},{"location":"developing/contributing/#developing-in-ocean4dvarnet","title":"Developing in Ocean4DVarNet","text":"<p>For contributing to the development of the Ocean4DVarNet packages, please follow these steps:</p> <ol> <li>Fork the Ocean4DVarNet repository on GitHub to your personal/organisation account. See the GitHub tutorial.</li> <li>Set up the development environment following the instructions in the Development Set-up section.</li> <li>Create a new branch for your developments, following the Branching Guidelines.</li> <li>Make your changes and ensure that your changes adhere to the Development Guidelines.</li> <li>Commit your changes and push your branch to your fork on GitHub.</li> <li>Open a Pull Request against the main branch of the original repository, set a PR title according to the PR Title Guidelines and fill in the Pull Request template.</li> <li>Request a review from maintainers or other contributors, which will follow the Code Review Process.</li> </ol>"},{"location":"developing/contributing/#code-review-process","title":"Code Review Process","text":"<p>The Ocean4DVarNet packages have a set of automated checks to enforce coding guidelines. These checks are run via GitHub Actions on every Pull Request. For security reasons, maintainers must review code changes before enabling automated checks.</p> <ol> <li>Ensure that all the Development Guidelines criteria are met before submitting a Pull Request.</li> <li>Request a review from maintainers or other contributors, noting that support is on a best-efforts basis.</li> <li>After an initial review, a maintainer will enable automated checks to run on the Pull Request.</li> <li>Reviewers may provide feedback or request changes to your contribution.</li> <li>Once approved, a maintainer will merge your Pull Request into the appropriate branch.</li> </ol>"},{"location":"developing/contributing/#code-of-conduct","title":"Code of conduct","text":"<p>Please follow the GitHub Code of Conduct for respectful collaboration.</p>"},{"location":"developing/contributing/#sources","title":"Sources","text":"<ul> <li>https://anemoi.readthedocs.io/en/latest/contributing/contributing.html#contributing</li> </ul>"},{"location":"developing/environment/","title":"Development environment set up","text":"<ol> <li>Create and activate a virtual environment with a python version &gt;=3.9, and &lt;3.12.</li> <li>Navigate to the repository you cloned and for which you want to install the dependencies. <pre><code>cd ocean4dvarnet\n</code></pre></li> <li>Install package dependencies <pre><code>pip install -e .\n</code></pre></li> <li>Install developement dependencies <pre><code>pip install -e .[dev]\n</code></pre></li> </ol>"},{"location":"developing/environment/#install-development","title":"Install development","text":"<p>To install the most recent development version, install from github:</p> <p>https://github.com/CIA-Oceanix/ocean4dvarnet.git git@github.com:CIA-Oceanix/ocean4dvarnet.git</p>"},{"location":"developing/fork/","title":"Development Set-up","text":""},{"location":"developing/fork/#setting-up-your-fork","title":"Setting Up Your Fork","text":"<p>When working with a fork, follow these steps to set up your local development environment:</p>"},{"location":"developing/fork/#fork-the-repository-create-your-own-copy-of-the-repository-on-github","title":"Fork the repository: Create your own copy of the repository on GitHub.**","text":"<ol> <li>On GitHub, navigate to the Ocean4DVarNet repository : https://github.com/CIA-Oceanix/ocean4dvarnet</li> <li>On the top-right corner of the page, click <code>Fork</code>. </li> <li>Select <code>create a new fork</code></li> <li>Under \"Owner\", select your login in the dropdown menu</li> <li>Under \"Repository Name\" don't use the same name as the upstream repository <code>ocean4dvarnet</code>, but customize it to distinguish it further. Keep a name including \"ocean4dvarnet\", ie <code>my-ocean4dvarnet</code></li> <li>In the \"Description\" field, type a description of your fork i.e \"Fork of the  ocean4dvarnet package\"</li> <li>Select <code>Copy the DEFAULT branch only.</code>. you only need to copy the default branch. If you do not select this option, all branches will be copied into the new fork.</li> <li>And then click on <code>Create fork</code> </li> </ol> <p>A this step, you have your own fork of the Ocean4DVarNet repository on you github account.</p> <p>More informations following this GitHub forking a repository tutorial.</p>"},{"location":"developing/fork/#clone-your-fork-download-your-forked-repository-to-your-local-machine","title":"Clone your fork: Download your forked repository to your local machine","text":"<ol> <li>On GitHub, navigate to your fork of the Ocean4DVarNet repository, click on <code>&lt;&gt;Code</code> and get the SSH url.</li> <li>Clone your fork of the Ocean4DVarNet repository on you local machine <pre><code>$ git clone https://github.com/YOUR-USERNAME/my-ocean4dvarnet\n&gt; Cloning into 'my-ocean4dvarnet'...\n&gt; remote: Enumerating objects: 99, done.\n&gt; remote: Counting objects: 100% (99/99), done.\n&gt; remote: Compressing objects: 100% (65/65), done.\n&gt; remote: Total 99 (delta 58), reused 70 (delta 33), pack-reused 0 (from 0)\n&gt; Receiving objects: 100% (99/99), 50.73 KiB | 16.91 MiB/s, done.\n&gt; Resolving deltas: 100% (58/58), done.\n</code></pre></li> <li>To go up created directory : <pre><code>$ cd my-ocean4dvarnet\n</code></pre></li> </ol> <p>More informations following this section of the tutorial.</p>"},{"location":"developing/fork/#keep-your-fork-up-to-date-configuring-git-to-sync-your-fork-with-the-upstream-repository","title":"Keep your fork Up to date : Configuring Git to sync your fork with the upstream repository","text":"<ol> <li>Display informations about the remote connexion of your fork repository : it is not connected to the upstream repository anymore, so it can not fetch updates. <pre><code>$ git remote -v\n&gt; origin    git@github.com:ebraux/my-ocean4dvarnet.git (fetch)\n&gt; origin    git@github.com:ebraux/my-ocean4dvarnet.git (push)\n</code></pre></li> <li>Add the upstream remote, to connect your local repository to the original repository <pre><code>$ git remote add upstream https://github.com/CIA-Oceanix/ocean4dvarnet.git\n</code></pre></li> <li>Verify the change with <pre><code>$ git remote -v\n&gt; origin    git@github.com:ebraux/my-ocean4dvarnet.git (fetch)\n&gt; origin    git@github.com:ebraux/my-ocean4dvarnet.git (push)\n&gt; upstream  https://github.com/CIA-Oceanix/ocean4dvarnet.git (fetch)\n&gt; upstream  https://github.com/CIA-Oceanix/ocean4dvarnet.git (push)\n</code></pre></li> <li>Prevent accidental pushes to upstream: After setting up your fork and configuring the original repository as an upstream remote, it\u2019s a good practice to prevent accidental pushes to the upstream repository. You can do this by explicitly setting the push URL of the upstream remote to no_push. <pre><code>$ git remote set-url --push upstream no_push\n</code></pre></li> <li>Verify the change with <pre><code>$ git remote -v\n&gt; origin      git@github.com:ebraux/my-ocean4dvarnet.git (fetch)\n&gt; origin      git@github.com:ebraux/my-ocean4dvarnet.git (push)\n&gt; upstream    https://github.com/CIA-Oceanix/ocean4dvarnet.git (fetch)\n&gt; upstream    no_push (push)\n</code></pre></li> </ol> <p>Now, you can keep your fork synced with the upstream repository with  <pre><code>$ git fetch upstream main\n&gt; From https://github.com/CIA-Oceanix/ocean4dvarnet\n&gt; * branch            main       -&gt; FETCH_HEAD\n</code></pre></p> <p>With this configuration, you can still fetch updates from the upstream repository but won\u2019t be able to accidentally push changes to it.</p> <p>More informations following this section.</p>"},{"location":"developing/guidelines/","title":"Guidelines","text":""},{"location":"developing/guidelines/#development-guidelines","title":"Development Guidelines","text":"<p>Please follow these development guidelines:</p> <ul> <li>Open an issue before starting a feature or bug fix to discuss the approach with maintainers and other users.</li> <li>Ensure high-quality code with appropriate tests (see Testing), documentation (see Documentation), linting, and style checks (see Code Quality and Style).</li> <li>Follow the Branching Guidelines.</li> <li>Follow the PR Title Guidelines.</li> </ul>"},{"location":"developing/guidelines/#branching-guidelines","title":"Branching Guidelines","text":"<ul> <li>Use feature branches for new features (e.g., feat/your-feature)</li> <li>Use fix branches for bug fixes (e.g., fix/your-bug)</li> <li>Use a descriptive name that indicates the purpose of the branch</li> <li>Keep branches up to date with main before opening a Pull Request</li> </ul>"},{"location":"developing/guidelines/#pr-and-commit-message-guidelines","title":"PR and commit Message Guidelines","text":"<p>PR and commit Message must follows the Conventional Commits guidelines, because our automated release process (release-please) relies on conventional commits to generate changelogs and determine version bumps automatically.</p> <p>Please ensure that it also follows theses guidelines :</p> <ol> <li>Reference relevant issue numbers in commit messages when applicable (e.g., \u201cfix: resolve data loading issue #123\u201d).</li> <li>Use present tense and imperative mood in commit messages (e.g., <code>Add feature</code> not <code>Added feature</code>).</li> </ol> <p>The PR title will become the squash commit message when merged to main</p> <p>For commits, we furthermore encourage you to make small, focused commits with clear and concise messages.</p> <p>The format of Conventional Commits guidelines. is: <code>type[(scope)][!]: description</code>. </p> <p><code>!</code> after the type/scope to indicate a breaking change.</p> <p>Common types include:</p> <ul> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>docs</code>: Documentation only.</li> <li><code>style</code>: Code style changes.</li> <li><code>refactor</code>: Code changes that neither fix bugs nor add features.</li> <li><code>test</code>: Adding or modifying tests.</li> <li><code>chore</code>: Maintenance tasks.</li> </ul> <p>For example:</p> <ul> <li><code>feat(train): add new loss function</code></li> <li><code>fix(plot): resolve node indexing bug</code></li> <li><code>docs(readme): update installation steps</code></li> <li><code>feat(models)!: change model input format (breaking change)</code></li> <li><code>refactor!: restructure project layout (breaking change)</code></li> </ul>"},{"location":"developing/ocean-code_env/","title":"Configuration d'un environnement de base","text":"<p>Installation d'un environnement Conda/Mamba/pip compl\u00e8tement autonome.</p> <p>Avec ipython pour pouvoir lancer les notebook</p>"},{"location":"developing/ocean-code_env/#installation-de-mamba","title":"Installation de mamba","text":"<ul> <li>D\u00e9finition du dossier racine pour d\u00e9ployer l'environnement et les d\u00e9veloppements:  ici cr\u00e9ation d'un dossier \"my-dev\" : <pre><code>cd $HOME\nmkdir ocean-code;\ncd ocean-code\n# ocean-code -&gt; OC\nOC_PATH=$PWD\nexport OC_PATH\necho $OC_PATH\n# /home/local-user/ocean-code\n</code></pre></li> </ul> <p>T\u00e9l\u00e9charger et installer miniforge3 <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-Linux-x86_64.sh -b -p \"${OC_PATH}/miniforge3\"\n</code></pre> Activer conda et mamba <pre><code>source \"${OC_PATH}/miniforge3/etc/profile.d/conda.sh\"\nsource \"${OC_PATH}/miniforge3/etc/profile.d/mamba.sh\"\n</code></pre></p> <ul> <li>Configurer l'environnement Conda par d\u00e9faut <pre><code>conda config --system --set channel_priority strict\nconda config --system --remove-key channels\nconda config --system --add channels  defaults\nconda config --system --prepend channels conda-forge\nconda config --system --remove channels  defaults\nconda config --system --append channels  nodefaults\n</code></pre></li> </ul>"},{"location":"developing/ocean-code_env/#creation-dun-environnement","title":"Cr\u00e9ation d'un environnement","text":"<ul> <li>Cr\u00e9er l'environnement : Attention, choisir la version de python qui correspond <pre><code>mamba create --name \"ocean-code\" python=3.12 -y\n</code></pre></li> <li>Installer pip <pre><code>mamba install pip\n</code></pre></li> <li>Afficher les versions de python avant activation de l'environnement <pre><code>python3 --version\n# Python 3.10.12\n\nwhich python3\n# /usr/bin/python3\n\npython3 -m pip --version\npip 25.0.1 from /home/local-user/.local/lib/python3.10/site-packages/pip (python 3.10)\n</code></pre></li> <li>Activer l'environnement <pre><code>mamba activate ocean-code\n</code></pre></li> <li>V\u00e9rifier que ce sont bien les version de python et pip de l'environnement qui sont utilis\u00e9es <pre><code>python3 --version\n# Python 3.12.9\n\nwhich python3\n# /home/local-user/my-dev/miniforge3/envs/ocean-code/bin/python3\n</code></pre></li> <li>Configurer Conda pour cet env <pre><code>conda config --env --set channel_priority flexible\nconda config --env --remove-key channels\nconda config --env --add channels  defaults\nconda config --env --prepend channels conda-forge\nconda config --env --remove channels  defaults\nconda config --env --append channels  nodefaults\n</code></pre></li> </ul>"},{"location":"developing/ocean-code_env/#configuration-de-pip","title":"Configuration de pip","text":""},{"location":"developing/ocean-code_env/#gestion-du-cache","title":"Gestion du cache","text":"<ul> <li>Configuration <pre><code>mkdir -p ${OC_PATH}/miniforge3/pip/cache\n\npip config --site set global.cache-dir  ${OC_PATH}/miniforge3/pip/cache\n# Writing to /home/local-user/my-dev/miniforge3/envs/4Dvarnet-test/pip.conf\n</code></pre></li> <li>V\u00e9rification <pre><code>pip cache dir\n# /home/local-user/my-dev/miniforge3/pip/cache\n</code></pre> <pre><code>[install]\nno-user = true\n</code></pre></li> </ul>"},{"location":"developing/ocean-code_env/#verification-de-la-configuration","title":"V\u00e9rification de la configuration","text":"<p>Les package et versions peuvent \u00eatre diff\u00e9rentes en fonction des version de python. Mais les listes des paquets install\u00e9 via mamba et pip doivent \u00eatr proches des listes ci dessous :</p> <ul> <li>mamba <pre><code>mamba list\n# # packages in environment at /home/ebraux/_LOCAL/mee/ocean/ocean-code/miniforge3/envs/ocean-code:\n# #\n# # Name                    Version                   Build  Channel\n# _libgcc_mutex             0.1                 conda_forge    conda-forge\n# _openmp_mutex             4.5                       2_gnu    conda-forge\n# bzip2                     1.0.8                h4bc722e_7    conda-forge\n# ca-certificates           2025.1.31            hbcca054_0    conda-forge\n# ld_impl_linux-64          2.43                 h712a8e2_4    conda-forge\n# libexpat                  2.6.4                h5888daf_0    conda-forge\n# libffi                    3.4.6                h2dba641_0    conda-forge\n# libgcc                    14.2.0               h767d61c_2    conda-forge\n# libgcc-ng                 14.2.0               h69a702a_2    conda-forge\n# libgomp                   14.2.0               h767d61c_2    conda-forge\n# liblzma                   5.6.4                hb9d3cd8_0    conda-forge\n# libnsl                    2.0.1                hd590300_0    conda-forge\n# libsqlite                 3.49.1               hee588c1_2    conda-forge\n# libuuid                   2.38.1               h0b41bf4_0    conda-forge\n# libxcrypt                 4.4.36               hd590300_1    conda-forge\n# libzlib                   1.3.1                hb9d3cd8_2    conda-forge\n# ncurses                   6.5                  h2d0b736_3    conda-forge\n# openssl                   3.4.1                h7b32b05_0    conda-forge\n# pip                       25.0.1             pyh8b19718_0    conda-forge\n# python                    3.12.9          h9e4cc4f_1_cpython    conda-forge\n# readline                  8.2                  h8c095d6_2    conda-forge\n# setuptools                75.8.2             pyhff2d567_0    conda-forge\n# tk                        8.6.13          noxft_h4845f30_101    conda-forge\n# tzdata                    2025b                h78e105d_0    conda-forge\n# wheel                     0.45.1             pyhd8ed1ab_1    conda-forge\n</code></pre></li> <li>pip <pre><code>pip freeze\n# setuptools==75.8.2\n# wheel==0.45.1\n</code></pre></li> </ul>"},{"location":"developing/ocean-code_env/#installation-de-cuda-pytorch","title":"Installation de CUDA + pytorch","text":"<p><code>mamba</code> permet d'installer les package manuellement, mais la bonne pratique est l'utilisation d'un fichier d'environnement : <code>environment.yaml</code></p> <ul> <li>cr\u00e9er le fichier <code>environment.yaml</code> <pre><code>channels:\n  - conda-forge\n  - pytorch\n  - nvidia\n  - nodefaults\ndependencies:\n  - pip\n  - pytorch::pytorch\n  - pytorch::pytorch-cuda\n  - pyinterp\n  - tqdm\n</code></pre></li> <li>et lancer la mise \u00e0 jour de l'environnement (prend du temps) <pre><code>mamba env update -f environment.yaml\n</code></pre></li> </ul> <p>Les messages de type ci dessous sont li\u00e9s \u00e0 un probl\u00e8me de connexion Internet, il suffit de relancer la commande. <pre><code>CondaSSLError: Encountered an SSL error. Most likely a certificate verification issue.\n...\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https://conda.anaconda.org/conda-forge/linux-64/libcufft-11.2.1.3-he02047a_2.conda&gt;\n...\nAn HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way.\n</code></pre></p>"},{"location":"developing/ocean-code_env/#jupyterlab-pour-les-notebook","title":"Jupyterlab pour les Notebook","text":"<pre><code>pip install jupyterlab\n</code></pre>"},{"location":"developing/package-development/","title":"Development","text":""},{"location":"developing/package-development/#package-description","title":"Package description","text":"<p>Package configuration : <code>pyproject.toml</code> file</p> <p>Packaging and publication tools :</p> <ul> <li>Build frondent :<code>build</code></li> <li>Build Backend : <code>SetupTool</code></li> <li>Publication tool : <code>Twine</code></li> </ul> <p>File structure :</p> <ul> <li>model.py : 4Dvarnet Model</li> <li>plot.py : plot function</li> </ul>"},{"location":"developing/package-development/#package-creation","title":"Package creation","text":"<ul> <li>Create Python virtual Environment <pre><code>python3 -m venv --prompt ocean4dvarnet venv\nsource venv/bin/activate\n</code></pre></li> <li>\"Frontend Builder\" installation : <code>build</code> <pre><code>pip install build\n</code></pre></li> <li>Package creation  <pre><code>python -m build\n# ...\n# Successfully built ocean4dvarnet-0.0.1.tar.gz and ocean4dvarnet-0.0.1-py3-none-any.whl\n</code></pre></li> </ul>"},{"location":"developing/package-development/#using-package-in-development-mode","title":"Using Package in development mode","text":"<ul> <li>Check Package <pre><code>ls  dist\n# ocean4dvarnet-0.0.1-py3-none-any.whl  ocean4dvarnet-0.0.1.tar.gz\n</code></pre></li> <li>Install package in deployment mode (\"editable\") <pre><code>pip install -e .\n# Successfully installed  ... ocean4dvarnet-0.0.1 ...\n#\u00a0certifi-2025.1.31 cftime-1.6.4.post1 netcdf4-1.7.2 numpy-2.2.3 ocean4dvarnet-0.0.1 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.1 six-1.17.0 tzdata-2025.1 xarray-2025.1.2\n</code></pre></li> <li>Check Package <pre><code>pip show ocean4dvarnet\n# Name: ocean4dvarnet\n# Version: 0.0.1\n# Summary: a computer program whose purpose is to apply deep learning schemes to dynamical systems and ocean remote sensing data.\n# Home-page: \n# Author: \n# Author-email: Ronan Fablet &lt;ronan.fablet@imt-atlantique.fr&gt;, Quentin Febvre &lt;quentin.febvre@ifremer.fr&gt;, Pierre Haslee &lt;author@example.com&gt;, DanieL Zhu &lt;daniel.zhu@imt-atlantique.fr&gt;, Hugo Georgenthum &lt;hugo.georgenthum@imt-atlantique.fr&gt;, Braux Emmanuel &lt;emmanuel.braux@imt-atlantique.fr&gt;\n# License: \n# Location: /home/ ... /ocean4dvarnet/venv/lib/python3.12/site-packages\n# Editable project location: /home/ ... /ocean4dvarnet\n# Requires: netcdf4, numpy, pandas, xarray\n# Required-by: \n</code></pre></li> <li>Uninstall Package <pre><code>pip uninstall ocean4dvarnet\n</code></pre></li> </ul>"},{"location":"developing/package-development/#package-publication","title":"Package Publication","text":"<p>The project has been created on https://test.pypi.org/, as <code>ocean4dvarnet</code>, with th module <code>twine</code>. </p> <p>To update the project, you need token with the correct rights, on test.pypi.org.</p> <p>To create a new version of the package, you need to modifiy the <code>pyproject.tom</code> file.</p> <ul> <li><code>twine</code> module installation <pre><code>pip install twine\n</code></pre></li> <li>Check package before upload <pre><code>twine check dist/*\n# Checking dist/ocean4dvarnet-0.0.1-py3-none-any.whl: PASSED\n# Checking dist/ocean4dvarnet-0.0.1.tar.gz: PASSED\n</code></pre></li> <li>Configure a token with the project modification rights <pre><code>export PYPI_TOKEN=\"pypi-AgENdGVz .....\"\n</code></pre></li> <li>Update the project <pre><code>twine upload --repository testpypi -p \"$PYPI_TOKEN\"   dist/*\n# Uploading distributions to https://test.pypi.org/legacy/\n# Uploading ocean4dvarnet-0.0.1-py3-none-any.whl\n# 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.2/27.2 kB \u2022 00:00 \u2022 43.2 MB/s\n# Uploading ocean4dvarnet-0.0.1.tar.gz\n# 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.2/27.2 kB \u2022 00:00 \u2022 48.6 MB/s\n# View at:\n# https://test.pypi.org/project/ocean4dvarnet/0.0.1/\n</code></pre></li> </ul>"},{"location":"developing/package-development/#additional-information","title":"Additional information","text":""},{"location":"developing/package-development/#package-creation-on-testpypiorg","title":"Package creation on test.pypi.org","text":"<p>Before running these command, you need to have created an account on Test.pypi.org, and a token with \"global rights\"</p> <ul> <li><code>twine</code>module installation <pre><code>pip install twine\n</code></pre></li> <li>Check package before upload <pre><code>twine check dist/*\n# Checking dist/ocean4dvarnet-0.0.1-py3-none-any.whl: PASSED\n# Checking dist/ocean4dvarnet-0.0.1.tar.gz: PASSED\n</code></pre></li> <li>Configure a twine with the PyPI global Token  <pre><code>export PYPI_TOKEN=\"pypi-AgENdGVz .....\"\n</code></pre></li> <li>Create the project <pre><code>twine upload \\\n  --verbose \\\n  --repository testpypi \\\n  -p \"$PYPI_TOKEN\" \\\n  dist/*\n\n# Uploading distributions to https://test.pypi.org/legacy/\n#\u00a0Uploading ocean4dvarnet-0.0.1-py3-none-any.whl\n# Uploading ocean4dvarnet-0.0.1.tar.gz\n# View at:\n# https://test.pypi.org/project/ocean4dvarnet/0.0.1/\n</code></pre></li> </ul>"},{"location":"developing/pr-workflow/","title":"Pull Request Workflow","text":"<p>When submitting Pull Requests (PRs), please follow these guidelines:</p> <ol> <li>Open a draft Pull Request early in your development process. This helps:<ul> <li>Make your work visible to other contributors.</li> <li>Get early feedback on your approach.</li> <li>Avoid duplicate efforts.</li> <li>Track progress on complex changes.</li> </ul> </li> <li>Fill the PR template completely, including:<ul> <li>Clear description of the changes.</li> <li>Link to related issues using GitHub keywords (e.g., \u201cFixes #123\u201d).</li> <li>List of notable changes.</li> <li>Any breaking changes or deprecations.</li> <li>Testing instructions if applicable.</li> </ul> </li> <li>Ensure the PR title follows the PR Title Guidelines, as this will become the squash commit message when merged to main.</li> <li>Keep your PR focused and of reasonable size:<ul> <li>One PR should address one concern.</li> <li>Split large changes into smaller, logical PRs.</li> <li>Update documentation along with code changes.</li> </ul> </li> <li>Before marking as ready for review:<ul> <li>Ensure all tests pass locally.</li> <li>Address any automated check failures.</li> <li>Review your own changes.</li> <li>Update based on any feedback received while in draft.</li> </ul> </li> <li>When ready for review:<ul> <li>Mark the PR as \u201cReady for Review\u201d</li> <li>Request reviews from appropriate team members.</li> <li>Be responsive to review comments.</li> <li>Update the PR description if significant changes are made.</li> </ul> </li> <li>After approval:<ul> <li>PRs are merged using squash merge to maintain a clean history.</li> <li>The squash commit message will use the PR title.</li> </ul> </li> </ol>"},{"location":"developing/pypi-packaging/","title":"Development","text":""},{"location":"developing/pypi-packaging/#package-description","title":"Package description","text":"<p>Package configuration : <code>pyproject.toml</code> file</p> <p>Packaging and publication tools :</p> <ul> <li>Build frondent :<code>build</code></li> <li>Build Backend : <code>SetupTool</code></li> <li>Publication tool : <code>Twine</code></li> </ul> <p>File structure :</p> <ul> <li>model.py : 4Dvarnet Model</li> <li>plot.py : plot function</li> </ul>"},{"location":"developing/pypi-packaging/#package-creation","title":"Package creation","text":"<ul> <li>Create Python virtual Environment <pre><code>python3 -m venv --prompt ocean4dvarnet venv\nsource venv/bin/activate\n</code></pre></li> <li>\"Frontend Builder\" installation : <code>build</code> <pre><code>pip install build\n</code></pre></li> <li>Package creation  <pre><code>python -m build\n# ...\n# Successfully built ocean4dvarnet-0.0.1.tar.gz and ocean4dvarnet-0.0.1-py3-none-any.whl\n</code></pre></li> </ul>"},{"location":"developing/pypi-packaging/#using-package-in-development-mode","title":"Using Package in development mode","text":"<ul> <li>Check Package <pre><code>ls  dist\n# ocean4dvarnet-0.0.1-py3-none-any.whl  ocean4dvarnet-0.0.1.tar.gz\n</code></pre></li> <li>Install package in deployment mode (\"editable\") <pre><code>pip install -e .\n# Successfully installed  ... ocean4dvarnet-0.0.1 ...\n#\u00a0certifi-2025.1.31 cftime-1.6.4.post1 netcdf4-1.7.2 numpy-2.2.3 ocean4dvarnet-0.0.1 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.1 six-1.17.0 tzdata-2025.1 xarray-2025.1.2\n</code></pre></li> <li>Check Package <pre><code>pip show ocean4dvarnet\n# Name: ocean4dvarnet\n# Version: 0.0.1\n# Summary: a computer program whose purpose is to apply deep learning schemes to dynamical systems and ocean remote sensing data.\n# Home-page: \n# Author: \n# Author-email: Ronan Fablet &lt;ronan.fablet@imt-atlantique.fr&gt;, Quentin Febvre &lt;quentin.febvre@ifremer.fr&gt;, Pierre Haslee &lt;author@example.com&gt;, DanieL Zhu &lt;daniel.zhu@imt-atlantique.fr&gt;, Hugo Georgenthum &lt;hugo.georgenthum@imt-atlantique.fr&gt;, Braux Emmanuel &lt;emmanuel.braux@imt-atlantique.fr&gt;\n# License: \n# Location: /home/ ... /ocean4dvarnet/venv/lib/python3.12/site-packages\n# Editable project location: /home/ ... /ocean4dvarnet\n# Requires: netcdf4, numpy, pandas, xarray\n# Required-by: \n</code></pre></li> <li>Uninstall Package <pre><code>pip uninstall ocean4dvarnet\n</code></pre></li> </ul>"},{"location":"developing/pypi-packaging/#package-publication","title":"Package Publication","text":"<p>The project has been created on https://test.pypi.org/, as <code>ocean4dvarnet</code>, with th module <code>twine</code>. </p> <p>To update the project, you need token with the correct rights, on test.pypi.org.</p> <p>To create a new version of the package, you need to modifiy the <code>pyproject.tom</code> file.</p> <ul> <li><code>twine</code> module installation <pre><code>pip install twine\n</code></pre></li> <li>Check package before upload <pre><code>twine check dist/*\n# Checking dist/ocean4dvarnet-0.0.1-py3-none-any.whl: PASSED\n# Checking dist/ocean4dvarnet-0.0.1.tar.gz: PASSED\n</code></pre></li> <li>Configure a token with the project modification rights <pre><code>export PYPI_TOKEN=\"pypi-AgENdGVz .....\"\n</code></pre></li> <li>Update the project <pre><code>twine upload --repository testpypi -p \"$PYPI_TOKEN\"   dist/*\n# Uploading distributions to https://test.pypi.org/legacy/\n# Uploading ocean4dvarnet-0.0.1-py3-none-any.whl\n# 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.2/27.2 kB \u2022 00:00 \u2022 43.2 MB/s\n# Uploading ocean4dvarnet-0.0.1.tar.gz\n# 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.2/27.2 kB \u2022 00:00 \u2022 48.6 MB/s\n# View at:\n# https://test.pypi.org/project/ocean4dvarnet/0.0.1/\n</code></pre></li> </ul>"},{"location":"developing/pypi-packaging/#additional-information","title":"Additional information","text":""},{"location":"developing/pypi-packaging/#package-creation-on-testpypiorg","title":"Package creation on test.pypi.org","text":"<p>Before running these command, you need to have created an account on Test.pypi.org, and a token with \"global rights\"</p> <ul> <li><code>twine</code>module installation <pre><code>pip install twine\n</code></pre></li> <li>Check package before upload <pre><code>twine check dist/*\n# Checking dist/ocean4dvarnet-0.0.1-py3-none-any.whl: PASSED\n# Checking dist/ocean4dvarnet-0.0.1.tar.gz: PASSED\n</code></pre></li> <li>Configure a twine with the PyPI global Token  <pre><code>export PYPI_TOKEN=\"pypi-AgENdGVz .....\"\n</code></pre></li> <li>Create the project <pre><code>twine upload \\\n  --verbose \\\n  --repository testpypi \\\n  -p \"$PYPI_TOKEN\" \\\n  dist/*\n\n# Uploading distributions to https://test.pypi.org/legacy/\n#\u00a0Uploading ocean4dvarnet-0.0.1-py3-none-any.whl\n# Uploading ocean4dvarnet-0.0.1.tar.gz\n# View at:\n# https://test.pypi.org/project/ocean4dvarnet/0.0.1/\n</code></pre></li> </ul>"},{"location":"developing/testing/","title":"Testing","text":"<p>To run the test suite after installing Ocean4DVarNet : </p> <p>Install dev dependencies (pylint, pytest, ...) in the models directory of the Ocean4DVarNet repository. <pre><code>pip install .[dev]\n</code></pre></p> <p>And the run :</p> <pre><code>pylint\n\npytest\n</code></pre>"},{"location":"developing/testing/#sources","title":"Sources","text":"<ul> <li>https://anemoi.readthedocs.io/en/latest/contributing/environment.html#setting-up-the-development-environment</li> <li>https://anemoi.readthedocs.io/en/latest/contributing/testing.html#testing-guidelines</li> </ul>"},{"location":"pkg-doc/data/","title":"data","text":"<p>This module provides data handling utilities for 4D-VarNet models.</p> <p>It includes classes and functions for creating datasets, augmenting data,  managing data loading pipelines, and reconstructing data from patches.  These utilities are designed to work seamlessly with PyTorch and xarray,  enabling efficient data preprocessing and loading for machine learning tasks.</p> <p>Classes:</p> Name Description <code>- XrDataset</code> <p>A PyTorch Dataset for extracting patches from xarray.DataArray objects.</p> <code>- XrConcatDataset</code> <p>A concatenation of multiple XrDatasets.</p> <code>- AugmentedDataset</code> <p>A dataset wrapper for applying data augmentation.</p> <code>- BaseDataModule</code> <p>A PyTorch Lightning DataModule for managing datasets and data loaders.</p> <code>- ConcatDataModule</code> <p>A DataModule for combining datasets from multiple domains.</p> <code>- RandValDataModule</code> <p>A DataModule for random splitting of training data into training and validation sets.</p> <p>Raises:</p> Type Description <code>-IncompleteScanConfiguration</code> <p>Raised when the scan configuration does not cover the entire domain.</p> <code>-DangerousDimOrdering</code> <p>Raised when the dimension ordering of the input data is incorrect.</p> Key Features <ul> <li>Patch extraction: Efficiently extract patches from large xarray.DataArray objects for training.</li> <li>Data augmentation: Support for augmenting datasets with noise and transformations.</li> <li>Reconstruction: Reconstruct the original data from extracted patches.</li> <li>Seamless integration: Designed to work with PyTorch Lightning for streamlined training pipelines.</li> </ul>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.AugmentedDataset","title":"<code>AugmentedDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset that applies data augmentation to an input dataset.</p> <p>Attributes:</p> Name Type Description <code>inp_ds</code> <code>Dataset</code> <p>The input dataset.</p> <code>aug_factor</code> <code>int</code> <p>The number of augmented copies to generate.</p> <code>aug_only</code> <code>bool</code> <p>Whether to include only augmented data.</p> <code>noise_sigma</code> <code>float</code> <p>Standard deviation of noise to add to augmented data.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class AugmentedDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset that applies data augmentation to an input dataset.\n\n    Attributes:\n        inp_ds (torch.utils.data.Dataset): The input dataset.\n        aug_factor (int): The number of augmented copies to generate.\n        aug_only (bool): Whether to include only augmented data.\n        noise_sigma (float): Standard deviation of noise to add to augmented data.\n    \"\"\"\n\n    def __init__(self, inp_ds, aug_factor, aug_only=False, noise_sigma=None):\n        \"\"\"\n        Initialize the AugmentedDataset.\n\n        Args:\n            inp_ds (torch.utils.data.Dataset): The input dataset.\n            aug_factor (int): The number of augmented copies to generate.\n            aug_only (bool, optional): Whether to include only augmented data.\n            noise_sigma (float, optional): Standard deviation of noise to add to augmented data.\n        \"\"\"\n        self.aug_factor = aug_factor\n        self.aug_only = aug_only\n        self.inp_ds = inp_ds\n        self.perm = np.random.permutation(len(self.inp_ds))\n        self.noise_sigma = noise_sigma\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of items in the dataset.\n\n        Returns:\n            int: Total number of items.\n        \"\"\"\n        return len(self.inp_ds) * (1 + self.aug_factor - int(self.aug_only))\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get an item from the dataset.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            TrainingItem: The requested item.\n        \"\"\"\n        if self.aug_only:\n            idx = idx + len(self.inp_ds)\n\n        if idx &lt; len(self.inp_ds):\n            return self.inp_ds[idx]\n\n        tgt_idx = idx % len(self.inp_ds)\n        perm_idx = tgt_idx\n        for _ in range(idx // len(self.inp_ds)):\n            perm_idx = self.perm[perm_idx]\n\n        item = self.inp_ds[tgt_idx]\n        perm_item = self.inp_ds[perm_idx]\n\n        noise = np.zeros_like(item.input, dtype=np.float32)\n        if self.noise_sigma is not None:\n            noise = np.random.randn(*item.input.shape).astype(np.float32) * self.noise_sigma\n\n        return item._replace(input=noise + np.where(np.isfinite(perm_item.input),\n                             item.tgt, np.full_like(item.tgt, np.nan)))\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.AugmentedDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item.</p> required <p>Returns:</p> Name Type Description <code>TrainingItem</code> <p>The requested item.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Get an item from the dataset.\n\n    Args:\n        idx (int): Index of the item.\n\n    Returns:\n        TrainingItem: The requested item.\n    \"\"\"\n    if self.aug_only:\n        idx = idx + len(self.inp_ds)\n\n    if idx &lt; len(self.inp_ds):\n        return self.inp_ds[idx]\n\n    tgt_idx = idx % len(self.inp_ds)\n    perm_idx = tgt_idx\n    for _ in range(idx // len(self.inp_ds)):\n        perm_idx = self.perm[perm_idx]\n\n    item = self.inp_ds[tgt_idx]\n    perm_item = self.inp_ds[perm_idx]\n\n    noise = np.zeros_like(item.input, dtype=np.float32)\n    if self.noise_sigma is not None:\n        noise = np.random.randn(*item.input.shape).astype(np.float32) * self.noise_sigma\n\n    return item._replace(input=noise + np.where(np.isfinite(perm_item.input),\n                         item.tgt, np.full_like(item.tgt, np.nan)))\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.AugmentedDataset.__init__","title":"<code>__init__(inp_ds, aug_factor, aug_only=False, noise_sigma=None)</code>","text":"<p>Initialize the AugmentedDataset.</p> <p>Parameters:</p> Name Type Description Default <code>inp_ds</code> <code>Dataset</code> <p>The input dataset.</p> required <code>aug_factor</code> <code>int</code> <p>The number of augmented copies to generate.</p> required <code>aug_only</code> <code>bool</code> <p>Whether to include only augmented data.</p> <code>False</code> <code>noise_sigma</code> <code>float</code> <p>Standard deviation of noise to add to augmented data.</p> <code>None</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __init__(self, inp_ds, aug_factor, aug_only=False, noise_sigma=None):\n    \"\"\"\n    Initialize the AugmentedDataset.\n\n    Args:\n        inp_ds (torch.utils.data.Dataset): The input dataset.\n        aug_factor (int): The number of augmented copies to generate.\n        aug_only (bool, optional): Whether to include only augmented data.\n        noise_sigma (float, optional): Standard deviation of noise to add to augmented data.\n    \"\"\"\n    self.aug_factor = aug_factor\n    self.aug_only = aug_only\n    self.inp_ds = inp_ds\n    self.perm = np.random.permutation(len(self.inp_ds))\n    self.noise_sigma = noise_sigma\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.AugmentedDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the total number of items in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Total number of items.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the total number of items in the dataset.\n\n    Returns:\n        int: Total number of items.\n    \"\"\"\n    return len(self.inp_ds) * (1 + self.aug_factor - int(self.aug_only))\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule","title":"<code>BaseDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A base data module for managing datasets and data loaders in PyTorch Lightning.</p> <p>Attributes:</p> Name Type Description <code>input_da</code> <code>DataArray</code> <p>The input data array.</p> <code>domains</code> <code>dict</code> <p>Dictionary of domain splits (train, val, test).</p> <code>xrds_kw</code> <code>dict</code> <p>Keyword arguments for XrDataset.</p> <code>dl_kw</code> <code>dict</code> <p>Keyword arguments for DataLoader.</p> <code>aug_kw</code> <code>dict</code> <p>Keyword arguments for AugmentedDataset.</p> <code>norm_stats</code> <code>tuple</code> <p>Normalization statistics (mean, std).</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class BaseDataModule(pl.LightningDataModule):\n    \"\"\"\n    A base data module for managing datasets and data loaders in PyTorch Lightning.\n\n    Attributes:\n        input_da (xarray.DataArray): The input data array.\n        domains (dict): Dictionary of domain splits (train, val, test).\n        xrds_kw (dict): Keyword arguments for XrDataset.\n        dl_kw (dict): Keyword arguments for DataLoader.\n        aug_kw (dict): Keyword arguments for AugmentedDataset.\n        norm_stats (tuple): Normalization statistics (mean, std).\n    \"\"\"\n\n    def __init__(self, input_da, domains, xrds_kw, dl_kw, aug_kw=None, norm_stats=None, **kwargs):\n        \"\"\"\n        Initialize the BaseDataModule.\n\n        Args:\n            input_da (xarray.DataArray): The input data array.\n            domains (dict): Dictionary of domain splits (train, val, test).\n            xrds_kw (dict): Keyword arguments for XrDataset.\n            dl_kw (dict): Keyword arguments for DataLoader.\n            aug_kw (dict, optional): Keyword arguments for AugmentedDataset.\n            norm_stats (tuple, optional): Normalization statistics (mean, std).\n        \"\"\"\n        super().__init__()\n        self.input_da = input_da\n        self.domains = domains\n        self.xrds_kw = xrds_kw\n        self.dl_kw = dl_kw\n        self.aug_kw = aug_kw if aug_kw is not None else {}\n        self._norm_stats = norm_stats\n\n        self.train_ds = None\n        self.val_ds = None\n        self.test_ds = None\n        self._post_fn = None\n\n    def norm_stats(self):\n        \"\"\"\n        Compute or retrieve normalization statistics (mean, std).\n\n        Returns:\n            tuple: Normalization statistics (mean, std).\n        \"\"\"\n        if self._norm_stats is None:\n            self._norm_stats = self.train_mean_std()\n            print(\"Norm stats\", self._norm_stats)\n        return self._norm_stats\n\n    def train_mean_std(self, variable='tgt'):\n        \"\"\"\n        Compute the mean and standard deviation of the training data.\n\n        Args:\n            variable (str, optional): Variable to compute statistics for.\n\n        Returns:\n            tuple: Mean and standard deviation.\n        \"\"\"\n        train_data = self.input_da.sel(self.xrds_kw.get('domain_limits', {})).sel(self.domains['train'])\n        return train_data.sel(variable=variable).pipe(lambda da: (da.mean().values.item(), da.std().values.item()))\n\n    def post_fn(self):\n        \"\"\"\n        Create a post-processing function for normalizing data.\n\n        Returns:\n            callable: Post-processing function.\n        \"\"\"\n        m, s = self.norm_stats()\n        def normalize(item): return (item - m) / s\n        return ft.partial(ft.reduce, lambda i, f: f(i), [\n            TrainingItem._make,\n            lambda item: item._replace(tgt=normalize(item.tgt)),\n            lambda item: item._replace(input=normalize(item.input)),\n        ])\n\n    def setup(self, stage='test'):\n        \"\"\"\n        Set up the datasets for training, validation, and testing.\n\n        Args:\n            stage (str, optional): Stage of the setup ('train', 'val', 'test').\n        \"\"\"\n        train_data = self.input_da.sel(self.domains['train'])\n        post_fn = self.post_fn()\n        self.train_ds = XrDataset(\n            train_data, **self.xrds_kw, postpro_fn=post_fn,\n        )\n        if self.aug_kw:\n            self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n        self.val_ds = XrDataset(\n            self.input_da.sel(self.domains['val']), **self.xrds_kw, postpro_fn=post_fn,\n        )\n        self.test_ds = XrDataset(\n            self.input_da.sel(self.domains['test']), **self.xrds_kw, postpro_fn=post_fn,\n        )\n\n    def train_dataloader(self):\n        \"\"\"\n        Create a DataLoader for the training dataset.\n\n        Returns:\n            DataLoader: Training DataLoader.\n        \"\"\"\n        return torch.utils.data.DataLoader(self.train_ds, shuffle=True, **self.dl_kw)\n\n    def val_dataloader(self):\n        \"\"\"\n        Create a DataLoader for the validation dataset.\n\n        Returns:\n            DataLoader: Validation DataLoader.\n        \"\"\"\n        return torch.utils.data.DataLoader(self.val_ds, shuffle=False, **self.dl_kw)\n\n    def test_dataloader(self):\n        \"\"\"\n        Create a DataLoader for the testing dataset.\n\n        Returns:\n            DataLoader: Testing DataLoader.\n        \"\"\"\n        return torch.utils.data.DataLoader(self.test_ds, shuffle=False, **self.dl_kw)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.__init__","title":"<code>__init__(input_da, domains, xrds_kw, dl_kw, aug_kw=None, norm_stats=None, **kwargs)</code>","text":"<p>Initialize the BaseDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>input_da</code> <code>DataArray</code> <p>The input data array.</p> required <code>domains</code> <code>dict</code> <p>Dictionary of domain splits (train, val, test).</p> required <code>xrds_kw</code> <code>dict</code> <p>Keyword arguments for XrDataset.</p> required <code>dl_kw</code> <code>dict</code> <p>Keyword arguments for DataLoader.</p> required <code>aug_kw</code> <code>dict</code> <p>Keyword arguments for AugmentedDataset.</p> <code>None</code> <code>norm_stats</code> <code>tuple</code> <p>Normalization statistics (mean, std).</p> <code>None</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __init__(self, input_da, domains, xrds_kw, dl_kw, aug_kw=None, norm_stats=None, **kwargs):\n    \"\"\"\n    Initialize the BaseDataModule.\n\n    Args:\n        input_da (xarray.DataArray): The input data array.\n        domains (dict): Dictionary of domain splits (train, val, test).\n        xrds_kw (dict): Keyword arguments for XrDataset.\n        dl_kw (dict): Keyword arguments for DataLoader.\n        aug_kw (dict, optional): Keyword arguments for AugmentedDataset.\n        norm_stats (tuple, optional): Normalization statistics (mean, std).\n    \"\"\"\n    super().__init__()\n    self.input_da = input_da\n    self.domains = domains\n    self.xrds_kw = xrds_kw\n    self.dl_kw = dl_kw\n    self.aug_kw = aug_kw if aug_kw is not None else {}\n    self._norm_stats = norm_stats\n\n    self.train_ds = None\n    self.val_ds = None\n    self.test_ds = None\n    self._post_fn = None\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.norm_stats","title":"<code>norm_stats()</code>","text":"<p>Compute or retrieve normalization statistics (mean, std).</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Normalization statistics (mean, std).</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def norm_stats(self):\n    \"\"\"\n    Compute or retrieve normalization statistics (mean, std).\n\n    Returns:\n        tuple: Normalization statistics (mean, std).\n    \"\"\"\n    if self._norm_stats is None:\n        self._norm_stats = self.train_mean_std()\n        print(\"Norm stats\", self._norm_stats)\n    return self._norm_stats\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.post_fn","title":"<code>post_fn()</code>","text":"<p>Create a post-processing function for normalizing data.</p> <p>Returns:</p> Name Type Description <code>callable</code> <p>Post-processing function.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def post_fn(self):\n    \"\"\"\n    Create a post-processing function for normalizing data.\n\n    Returns:\n        callable: Post-processing function.\n    \"\"\"\n    m, s = self.norm_stats()\n    def normalize(item): return (item - m) / s\n    return ft.partial(ft.reduce, lambda i, f: f(i), [\n        TrainingItem._make,\n        lambda item: item._replace(tgt=normalize(item.tgt)),\n        lambda item: item._replace(input=normalize(item.input)),\n    ])\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.setup","title":"<code>setup(stage='test')</code>","text":"<p>Set up the datasets for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage of the setup ('train', 'val', 'test').</p> <code>'test'</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def setup(self, stage='test'):\n    \"\"\"\n    Set up the datasets for training, validation, and testing.\n\n    Args:\n        stage (str, optional): Stage of the setup ('train', 'val', 'test').\n    \"\"\"\n    train_data = self.input_da.sel(self.domains['train'])\n    post_fn = self.post_fn()\n    self.train_ds = XrDataset(\n        train_data, **self.xrds_kw, postpro_fn=post_fn,\n    )\n    if self.aug_kw:\n        self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n    self.val_ds = XrDataset(\n        self.input_da.sel(self.domains['val']), **self.xrds_kw, postpro_fn=post_fn,\n    )\n    self.test_ds = XrDataset(\n        self.input_da.sel(self.domains['test']), **self.xrds_kw, postpro_fn=post_fn,\n    )\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Create a DataLoader for the testing dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <p>Testing DataLoader.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def test_dataloader(self):\n    \"\"\"\n    Create a DataLoader for the testing dataset.\n\n    Returns:\n        DataLoader: Testing DataLoader.\n    \"\"\"\n    return torch.utils.data.DataLoader(self.test_ds, shuffle=False, **self.dl_kw)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create a DataLoader for the training dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <p>Training DataLoader.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def train_dataloader(self):\n    \"\"\"\n    Create a DataLoader for the training dataset.\n\n    Returns:\n        DataLoader: Training DataLoader.\n    \"\"\"\n    return torch.utils.data.DataLoader(self.train_ds, shuffle=True, **self.dl_kw)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.train_mean_std","title":"<code>train_mean_std(variable='tgt')</code>","text":"<p>Compute the mean and standard deviation of the training data.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>Variable to compute statistics for.</p> <code>'tgt'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Mean and standard deviation.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def train_mean_std(self, variable='tgt'):\n    \"\"\"\n    Compute the mean and standard deviation of the training data.\n\n    Args:\n        variable (str, optional): Variable to compute statistics for.\n\n    Returns:\n        tuple: Mean and standard deviation.\n    \"\"\"\n    train_data = self.input_da.sel(self.xrds_kw.get('domain_limits', {})).sel(self.domains['train'])\n    return train_data.sel(variable=variable).pipe(lambda da: (da.mean().values.item(), da.std().values.item()))\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.BaseDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create a DataLoader for the validation dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <p>Validation DataLoader.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def val_dataloader(self):\n    \"\"\"\n    Create a DataLoader for the validation dataset.\n\n    Returns:\n        DataLoader: Validation DataLoader.\n    \"\"\"\n    return torch.utils.data.DataLoader(self.val_ds, shuffle=False, **self.dl_kw)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.ConcatDataModule","title":"<code>ConcatDataModule</code>","text":"<p>               Bases: <code>BaseDataModule</code></p> <p>A data module for concatenating datasets from multiple domains.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class ConcatDataModule(BaseDataModule):\n    \"\"\"A data module for concatenating datasets from multiple domains.\"\"\"\n\n    def train_mean_std(self):\n        \"\"\"\n        Compute the mean and standard deviation of the training data across domains.\n\n        Returns:\n            tuple: Mean and standard deviation.\n        \"\"\"\n        sum, count = 0, 0\n        train_data = self.input_da.sel(self.xrds_kw.get('domain_limits', {}))\n        for domain in self.domains['train']:\n            _sum, _count = train_data.sel(domain).sel(variable='tgt').pipe(\n                lambda da: (da.sum(), da.pipe(np.isfinite).sum())\n            )\n            sum += _sum\n            count += _count\n\n        mean = sum / count\n        sum = 0\n        for domain in self.domains['train']:\n            _sum = train_data.sel(domain).sel(variable='tgt').pipe(lambda da: da - mean).pipe(np.square).sum()\n            sum += _sum\n        std = (sum / count)**0.5\n        return mean.values.item(), std.values.item()\n\n    def setup(self, stage='test'):\n        \"\"\"\n        Set up the datasets for training, validation, and testing.\n\n        Args:\n            stage (str, optional): Stage of the setup ('train', 'val', 'test').\n        \"\"\"\n        post_fn = self.post_fn()\n        self.train_ds = XrConcatDataset([\n            XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n            for domain in self.domains['train']\n        ])\n        if self.aug_factor &gt;= 1:\n            self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n        self.val_ds = XrConcatDataset([\n            XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n            for domain in self.domains['val']\n        ])\n        self.test_ds = XrConcatDataset([\n            XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n            for domain in self.domains['test']\n        ])\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.ConcatDataModule.setup","title":"<code>setup(stage='test')</code>","text":"<p>Set up the datasets for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage of the setup ('train', 'val', 'test').</p> <code>'test'</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def setup(self, stage='test'):\n    \"\"\"\n    Set up the datasets for training, validation, and testing.\n\n    Args:\n        stage (str, optional): Stage of the setup ('train', 'val', 'test').\n    \"\"\"\n    post_fn = self.post_fn()\n    self.train_ds = XrConcatDataset([\n        XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n        for domain in self.domains['train']\n    ])\n    if self.aug_factor &gt;= 1:\n        self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n    self.val_ds = XrConcatDataset([\n        XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n        for domain in self.domains['val']\n    ])\n    self.test_ds = XrConcatDataset([\n        XrDataset(self.input_da.sel(domain), **self.xrds_kw, postpro_fn=post_fn,)\n        for domain in self.domains['test']\n    ])\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.ConcatDataModule.train_mean_std","title":"<code>train_mean_std()</code>","text":"<p>Compute the mean and standard deviation of the training data across domains.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Mean and standard deviation.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def train_mean_std(self):\n    \"\"\"\n    Compute the mean and standard deviation of the training data across domains.\n\n    Returns:\n        tuple: Mean and standard deviation.\n    \"\"\"\n    sum, count = 0, 0\n    train_data = self.input_da.sel(self.xrds_kw.get('domain_limits', {}))\n    for domain in self.domains['train']:\n        _sum, _count = train_data.sel(domain).sel(variable='tgt').pipe(\n            lambda da: (da.sum(), da.pipe(np.isfinite).sum())\n        )\n        sum += _sum\n        count += _count\n\n    mean = sum / count\n    sum = 0\n    for domain in self.domains['train']:\n        _sum = train_data.sel(domain).sel(variable='tgt').pipe(lambda da: da - mean).pipe(np.square).sum()\n        sum += _sum\n    std = (sum / count)**0.5\n    return mean.values.item(), std.values.item()\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.DangerousDimOrdering","title":"<code>DangerousDimOrdering</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the dimension ordering of the input data is incorrect.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class DangerousDimOrdering(Exception):\n    \"\"\"Exception raised when the dimension ordering of the input data is incorrect.\"\"\"\n    pass\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.IncompleteScanConfiguration","title":"<code>IncompleteScanConfiguration</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the scan configuration does not cover the entire domain.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class IncompleteScanConfiguration(Exception):\n    \"\"\"Exception raised when the scan configuration does not cover the entire domain.\"\"\"\n    pass\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.RandValDataModule","title":"<code>RandValDataModule</code>","text":"<p>               Bases: <code>BaseDataModule</code></p> <p>A data module that randomly splits the training data into training and validation sets.</p> <p>Attributes:</p> Name Type Description <code>val_prop</code> <code>float</code> <p>Proportion of data to use for validation.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class RandValDataModule(BaseDataModule):\n    \"\"\"\n    A data module that randomly splits the training data into training and validation sets.\n\n    Attributes:\n        val_prop (float): Proportion of data to use for validation.\n    \"\"\"\n\n    def __init__(self, val_prop, *args, **kwargs):\n        \"\"\"\n        Initialize the RandValDataModule.\n\n        Args:\n            val_prop (float): Proportion of data to use for validation.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.val_prop = val_prop\n\n    def setup(self, stage='test'):\n        \"\"\"\n        Set up the datasets for training, validation, and testing.\n\n        Args:\n            stage (str, optional): Stage of the setup ('train', 'val', 'test').\n        \"\"\"\n        post_fn = self.post_fn()\n        train_ds = XrDataset(self.input_da.sel(self.domains['train']), **self.xrds_kw, postpro_fn=post_fn,)\n        n_val = int(self.val_prop * len(train_ds))\n        n_train = len(train_ds) - n_val\n        self.train_ds, self.val_ds = torch.utils.data.random_split(train_ds, [n_train, n_val])\n\n        if self.aug_factor &gt; 1:\n            self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n        self.test_ds = XrDataset(self.input_da.sel(self.domains['test']), **self.xrds_kw, postpro_fn=post_fn,)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.RandValDataModule.__init__","title":"<code>__init__(val_prop, *args, **kwargs)</code>","text":"<p>Initialize the RandValDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>val_prop</code> <code>float</code> <p>Proportion of data to use for validation.</p> required Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __init__(self, val_prop, *args, **kwargs):\n    \"\"\"\n    Initialize the RandValDataModule.\n\n    Args:\n        val_prop (float): Proportion of data to use for validation.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.val_prop = val_prop\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.RandValDataModule.setup","title":"<code>setup(stage='test')</code>","text":"<p>Set up the datasets for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage of the setup ('train', 'val', 'test').</p> <code>'test'</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def setup(self, stage='test'):\n    \"\"\"\n    Set up the datasets for training, validation, and testing.\n\n    Args:\n        stage (str, optional): Stage of the setup ('train', 'val', 'test').\n    \"\"\"\n    post_fn = self.post_fn()\n    train_ds = XrDataset(self.input_da.sel(self.domains['train']), **self.xrds_kw, postpro_fn=post_fn,)\n    n_val = int(self.val_prop * len(train_ds))\n    n_train = len(train_ds) - n_val\n    self.train_ds, self.val_ds = torch.utils.data.random_split(train_ds, [n_train, n_val])\n\n    if self.aug_factor &gt; 1:\n        self.train_ds = AugmentedDataset(self.train_ds, **self.aug_kw)\n\n    self.test_ds = XrDataset(self.input_da.sel(self.domains['test']), **self.xrds_kw, postpro_fn=post_fn,)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrConcatDataset","title":"<code>XrConcatDataset</code>","text":"<p>               Bases: <code>ConcatDataset</code></p> <p>A concatenation of multiple XrDatasets.</p> <p>This class allows combining multiple datasets into one for training or evaluation.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class XrConcatDataset(torch.utils.data.ConcatDataset):\n    \"\"\"\n    A concatenation of multiple XrDatasets.\n\n    This class allows combining multiple datasets into one for training or evaluation.\n    \"\"\"\n\n    def reconstruct(self, batches, weight=None):\n        \"\"\"\n        Reconstruct the original data arrays from batches.\n\n        Args:\n            batches (list): List of batches.\n            weight (np.ndarray, optional): Weighting for overlapping patches.\n\n        Returns:\n            list: List of reconstructed xarray.DataArray objects.\n        \"\"\"\n        items_iter = itertools.chain(*batches)\n        rec_das = []\n        for ds in self.datasets:\n            ds_items = list(itertools.islice(items_iter, len(ds)))\n            rec_das.append(ds.reconstruct_from_items(ds_items, weight))\n\n        return rec_das\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrConcatDataset.reconstruct","title":"<code>reconstruct(batches, weight=None)</code>","text":"<p>Reconstruct the original data arrays from batches.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list</code> <p>List of batches.</p> required <code>weight</code> <code>ndarray</code> <p>Weighting for overlapping patches.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of reconstructed xarray.DataArray objects.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def reconstruct(self, batches, weight=None):\n    \"\"\"\n    Reconstruct the original data arrays from batches.\n\n    Args:\n        batches (list): List of batches.\n        weight (np.ndarray, optional): Weighting for overlapping patches.\n\n    Returns:\n        list: List of reconstructed xarray.DataArray objects.\n    \"\"\"\n    items_iter = itertools.chain(*batches)\n    rec_das = []\n    for ds in self.datasets:\n        ds_items = list(itertools.islice(items_iter, len(ds)))\n        rec_das.append(ds.reconstruct_from_items(ds_items, weight))\n\n    return rec_das\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset","title":"<code>XrDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset based on an xarray.DataArray with on-the-fly slicing.</p> <p>This class allows efficient extraction of patches from an xarray.DataArray for training machine learning models.</p> Usage <p>If you want to be able to reconstruct the input, the input xr.DataArray should: - Have coordinates. - Have the last dims correspond to the patch dims in the same order. - Have, for each dim of patch_dim, (size(dim) - patch_dim(dim)) divisible by stride(dim).</p> <p>The batches passed to self.reconstruct should: - Have the last dims correspond to the patch dims in the same order.</p> <p>Attributes:</p> Name Type Description <code>da</code> <code>DataArray</code> <p>The input data array.</p> <code>patch_dims</code> <code>dict</code> <p>Dimensions and sizes of patches to extract.</p> <code>domain_limits</code> <code>dict</code> <p>Limits for selecting a subset of the domain.</p> <code>strides</code> <code>dict</code> <p>Strides for patch extraction.</p> <code>check_full_scan</code> <code>bool</code> <p>Whether to check if the entire domain is scanned.</p> <code>check_dim_order</code> <code>bool</code> <p>Whether to check the dimension ordering.</p> <code>postpro_fn</code> <code>callable</code> <p>A function for post-processing extracted patches.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>class XrDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A PyTorch Dataset based on an xarray.DataArray with on-the-fly slicing.\n\n    This class allows efficient extraction of patches from an xarray.DataArray\n    for training machine learning models.\n\n    Usage:\n        If you want to be able to reconstruct the input, the input xr.DataArray should:\n        - Have coordinates.\n        - Have the last dims correspond to the patch dims in the same order.\n        - Have, for each dim of patch_dim, (size(dim) - patch_dim(dim)) divisible by stride(dim).\n\n        The batches passed to self.reconstruct should:\n        - Have the last dims correspond to the patch dims in the same order.\n\n\n    Attributes:\n        da (xarray.DataArray): The input data array.\n        patch_dims (dict): Dimensions and sizes of patches to extract.\n        domain_limits (dict): Limits for selecting a subset of the domain.\n        strides (dict): Strides for patch extraction.\n        check_full_scan (bool): Whether to check if the entire domain is scanned.\n        check_dim_order (bool): Whether to check the dimension ordering.\n        postpro_fn (callable): A function for post-processing extracted patches.\n\n    \"\"\"\n\n    def __init__(\n            self, da, patch_dims, domain_limits=None, strides=None,\n            check_full_scan=False, check_dim_order=False,\n            postpro_fn=None\n    ):\n        \"\"\"\n        Initialize the XrDataset.\n\n        Args:\n            da (xarray.DataArray): Input data, with patch dims at the end in the dim orders\n            patch_dims (dict):  da dimension and sizes of patches to extract.\n            domain_limits (dict, optional): da dimension slices of domain, to Limits for selecting\n                                            a subset of the domain. for patch extractions\n            strides (dict, optional): dims to strides size for patch extraction.(default to one)\n            check_full_scan (bool, optional): if True raise an error if the whole domain is not scanned by the patch.\n            check_dim_order (bool, optional): Whether to check the dimension ordering.\n            postpro_fn (callable, optional): A function for post-processing extracted patches.\n        \"\"\"\n        super().__init__()\n        self.return_coords = False\n        self.postpro_fn = postpro_fn\n        self.da = da.sel(**(domain_limits or {}))\n        self.patch_dims = patch_dims\n        self.strides = strides or {}\n        da_dims = dict(zip(self.da.dims, self.da.shape))\n        self.ds_size = {\n            dim: max((da_dims[dim] - patch_dims[dim]) // self.strides.get(dim, 1) + 1, 0)\n            for dim in patch_dims\n        }\n\n        if check_full_scan:\n            for dim in patch_dims:\n                if (da_dims[dim] - self.patch_dims[dim]) % self.strides.get(dim, 1) != 0:\n                    raise IncompleteScanConfiguration(\n                        f\"\"\"\n                        Incomplete scan in dimension dim {dim}:\n                        dataarray shape on this dim {da_dims[dim]}\n                        patch_size along this dim {self.patch_dims[dim]}\n                        stride along this dim {self.strides.get(dim, 1)}\n                        [shape - patch_size] should be divisible by stride\n                        \"\"\"\n                    )\n\n        if check_dim_order:\n            for dim in patch_dims:\n                if not '#'.join(da.dims).endswith('#'.join(list(patch_dims))):\n                    raise DangerousDimOrdering(\n                        f\"\"\"\n                        input dataarray's dims should end with patch_dims\n                        dataarray's dim {da.dims}:\n                        patch_dims {list(patch_dims)}\n                        \"\"\"\n                    )\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of patches in the dataset.\n\n        Returns:\n            int: Number of patches.\n        \"\"\"\n        size = 1\n        for v in self.ds_size.values():\n            size *= v\n        return size\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the dataset.\n\n        Yields:\n            Patch data for each index.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    def get_coords(self):\n        \"\"\"\n        Get the coordinates of all patches in the dataset.\n\n        Returns:\n            list: List of coordinates for each patch.\n        \"\"\"\n        self.return_coords = True\n        coords = []\n        try:\n            for i in range(len(self)):\n                 coords.append(self[i])\n        finally:\n            self.return_coords = False\n            return coords\n\n    def __getitem__(self, item):\n        \"\"\"\n        Get a specific patch by index.\n\n        Args:\n            item (int): Index of the patch.\n\n        Returns:\n            Patch data or coordinates, depending on the mode.\n        \"\"\"\n        sl = {\n            dim: slice(self.strides.get(dim, 1) * idx,\n                       self.strides.get(dim, 1) * idx + self.patch_dims[dim])\n            for dim, idx in zip(self.ds_size.keys(),\n                                np.unravel_index(item, tuple(self.ds_size.values())))\n        }\n        item = self.da.isel(**sl)\n\n        if self.return_coords:\n            return item.coords.to_dataset()[list(self.patch_dims)]\n\n        item = item.data.astype(np.float32)\n        if self.postpro_fn is not None:\n            return self.postpro_fn(item)\n        return item\n\n    def reconstruct(self, batches, weight=None):\n        \"\"\"\n        Reconstruct the original data array from patches.\n\n        Takes as input a list of np.ndarray of dimensions (b, *, *patch_dims).\n\n        Args:\n            batches (list): List of patches (torch tensor) corresponding to batches without shuffle.\n            weight (np.ndarray, optional): Tensor of size patch_dims corresponding to the weight of a prediction \n                depending on the position on the patch (default to ones everywhere). Overlapping patches will\n                be averaged with weighting.\n\n        Returns:\n            xarray.DataArray: Reconstructed data array. A stitched xarray.DataArray with the coords of patch_dims.\n        \"\"\"\n        items = list(itertools.chain(*batches))\n        return self.reconstruct_from_items(items, weight)\n\n    def reconstruct_from_items(self, items, weight=None):\n        \"\"\"\n        Reconstruct the original data array from individual items.\n\n        Args:\n            items (list): List of individual patches.\n            weight (np.ndarray, optional): Weighting for overlapping patches.\n\n        Returns:\n            xarray.DataArray: Reconstructed data array.\n        \"\"\"\n        if weight is None:\n            weight = np.ones(list(self.patch_dims.values()))\n        w = xr.DataArray(weight, dims=list(self.patch_dims.keys()))\n\n        coords = self.get_coords()\n\n        new_dims = [f'v{i}' for i in range(len(items[0].shape) - len(coords[0].dims))]\n        dims = new_dims + list(coords[0].dims)\n\n        das = [xr.DataArray(it.numpy(), dims=dims, coords=co.coords)\n               for it, co in zip(items, coords)]\n\n        da_shape = dict(zip(coords[0].dims, self.da.shape[-len(coords[0].dims):]))\n        new_shape = dict(zip(new_dims, items[0].shape[:len(new_dims)]))\n\n        rec_da = xr.DataArray(\n            np.zeros([*new_shape.values(), *da_shape.values()]),\n            dims=dims,\n            coords={d: self.da[d] for d in self.patch_dims}\n        )\n        count_da = xr.zeros_like(rec_da)\n\n        for da in das:\n            rec_da.loc[da.coords] = rec_da.sel(da.coords) + da * w\n            count_da.loc[da.coords] = count_da.sel(da.coords) + w\n\n        return rec_da / count_da\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Get a specific patch by index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int</code> <p>Index of the patch.</p> required <p>Returns:</p> Type Description <p>Patch data or coordinates, depending on the mode.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Get a specific patch by index.\n\n    Args:\n        item (int): Index of the patch.\n\n    Returns:\n        Patch data or coordinates, depending on the mode.\n    \"\"\"\n    sl = {\n        dim: slice(self.strides.get(dim, 1) * idx,\n                   self.strides.get(dim, 1) * idx + self.patch_dims[dim])\n        for dim, idx in zip(self.ds_size.keys(),\n                            np.unravel_index(item, tuple(self.ds_size.values())))\n    }\n    item = self.da.isel(**sl)\n\n    if self.return_coords:\n        return item.coords.to_dataset()[list(self.patch_dims)]\n\n    item = item.data.astype(np.float32)\n    if self.postpro_fn is not None:\n        return self.postpro_fn(item)\n    return item\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.__init__","title":"<code>__init__(da, patch_dims, domain_limits=None, strides=None, check_full_scan=False, check_dim_order=False, postpro_fn=None)</code>","text":"<p>Initialize the XrDataset.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>Input data, with patch dims at the end in the dim orders</p> required <code>patch_dims</code> <code>dict</code> <p>da dimension and sizes of patches to extract.</p> required <code>domain_limits</code> <code>dict</code> <p>da dimension slices of domain, to Limits for selecting                             a subset of the domain. for patch extractions</p> <code>None</code> <code>strides</code> <code>dict</code> <p>dims to strides size for patch extraction.(default to one)</p> <code>None</code> <code>check_full_scan</code> <code>bool</code> <p>if True raise an error if the whole domain is not scanned by the patch.</p> <code>False</code> <code>check_dim_order</code> <code>bool</code> <p>Whether to check the dimension ordering.</p> <code>False</code> <code>postpro_fn</code> <code>callable</code> <p>A function for post-processing extracted patches.</p> <code>None</code> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __init__(\n        self, da, patch_dims, domain_limits=None, strides=None,\n        check_full_scan=False, check_dim_order=False,\n        postpro_fn=None\n):\n    \"\"\"\n    Initialize the XrDataset.\n\n    Args:\n        da (xarray.DataArray): Input data, with patch dims at the end in the dim orders\n        patch_dims (dict):  da dimension and sizes of patches to extract.\n        domain_limits (dict, optional): da dimension slices of domain, to Limits for selecting\n                                        a subset of the domain. for patch extractions\n        strides (dict, optional): dims to strides size for patch extraction.(default to one)\n        check_full_scan (bool, optional): if True raise an error if the whole domain is not scanned by the patch.\n        check_dim_order (bool, optional): Whether to check the dimension ordering.\n        postpro_fn (callable, optional): A function for post-processing extracted patches.\n    \"\"\"\n    super().__init__()\n    self.return_coords = False\n    self.postpro_fn = postpro_fn\n    self.da = da.sel(**(domain_limits or {}))\n    self.patch_dims = patch_dims\n    self.strides = strides or {}\n    da_dims = dict(zip(self.da.dims, self.da.shape))\n    self.ds_size = {\n        dim: max((da_dims[dim] - patch_dims[dim]) // self.strides.get(dim, 1) + 1, 0)\n        for dim in patch_dims\n    }\n\n    if check_full_scan:\n        for dim in patch_dims:\n            if (da_dims[dim] - self.patch_dims[dim]) % self.strides.get(dim, 1) != 0:\n                raise IncompleteScanConfiguration(\n                    f\"\"\"\n                    Incomplete scan in dimension dim {dim}:\n                    dataarray shape on this dim {da_dims[dim]}\n                    patch_size along this dim {self.patch_dims[dim]}\n                    stride along this dim {self.strides.get(dim, 1)}\n                    [shape - patch_size] should be divisible by stride\n                    \"\"\"\n                )\n\n    if check_dim_order:\n        for dim in patch_dims:\n            if not '#'.join(da.dims).endswith('#'.join(list(patch_dims))):\n                raise DangerousDimOrdering(\n                    f\"\"\"\n                    input dataarray's dims should end with patch_dims\n                    dataarray's dim {da.dims}:\n                    patch_dims {list(patch_dims)}\n                    \"\"\"\n                )\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the dataset.</p> <p>Yields:</p> Type Description <p>Patch data for each index.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the dataset.\n\n    Yields:\n        Patch data for each index.\n    \"\"\"\n    for i in range(len(self)):\n        yield self[i]\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the total number of patches in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of patches.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the total number of patches in the dataset.\n\n    Returns:\n        int: Number of patches.\n    \"\"\"\n    size = 1\n    for v in self.ds_size.values():\n        size *= v\n    return size\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.get_coords","title":"<code>get_coords()</code>","text":"<p>Get the coordinates of all patches in the dataset.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of coordinates for each patch.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def get_coords(self):\n    \"\"\"\n    Get the coordinates of all patches in the dataset.\n\n    Returns:\n        list: List of coordinates for each patch.\n    \"\"\"\n    self.return_coords = True\n    coords = []\n    try:\n        for i in range(len(self)):\n             coords.append(self[i])\n    finally:\n        self.return_coords = False\n        return coords\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.reconstruct","title":"<code>reconstruct(batches, weight=None)</code>","text":"<p>Reconstruct the original data array from patches.</p> <p>Takes as input a list of np.ndarray of dimensions (b, , patch_dims).</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list</code> <p>List of patches (torch tensor) corresponding to batches without shuffle.</p> required <code>weight</code> <code>ndarray</code> <p>Tensor of size patch_dims corresponding to the weight of a prediction  depending on the position on the patch (default to ones everywhere). Overlapping patches will be averaged with weighting.</p> <code>None</code> <p>Returns:</p> Type Description <p>xarray.DataArray: Reconstructed data array. A stitched xarray.DataArray with the coords of patch_dims.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def reconstruct(self, batches, weight=None):\n    \"\"\"\n    Reconstruct the original data array from patches.\n\n    Takes as input a list of np.ndarray of dimensions (b, *, *patch_dims).\n\n    Args:\n        batches (list): List of patches (torch tensor) corresponding to batches without shuffle.\n        weight (np.ndarray, optional): Tensor of size patch_dims corresponding to the weight of a prediction \n            depending on the position on the patch (default to ones everywhere). Overlapping patches will\n            be averaged with weighting.\n\n    Returns:\n        xarray.DataArray: Reconstructed data array. A stitched xarray.DataArray with the coords of patch_dims.\n    \"\"\"\n    items = list(itertools.chain(*batches))\n    return self.reconstruct_from_items(items, weight)\n</code></pre>"},{"location":"pkg-doc/data/#ocean4dvarnet.data.XrDataset.reconstruct_from_items","title":"<code>reconstruct_from_items(items, weight=None)</code>","text":"<p>Reconstruct the original data array from individual items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list</code> <p>List of individual patches.</p> required <code>weight</code> <code>ndarray</code> <p>Weighting for overlapping patches.</p> <code>None</code> <p>Returns:</p> Type Description <p>xarray.DataArray: Reconstructed data array.</p> Source code in <code>ocean4dvarnet/data.py</code> <pre><code>def reconstruct_from_items(self, items, weight=None):\n    \"\"\"\n    Reconstruct the original data array from individual items.\n\n    Args:\n        items (list): List of individual patches.\n        weight (np.ndarray, optional): Weighting for overlapping patches.\n\n    Returns:\n        xarray.DataArray: Reconstructed data array.\n    \"\"\"\n    if weight is None:\n        weight = np.ones(list(self.patch_dims.values()))\n    w = xr.DataArray(weight, dims=list(self.patch_dims.keys()))\n\n    coords = self.get_coords()\n\n    new_dims = [f'v{i}' for i in range(len(items[0].shape) - len(coords[0].dims))]\n    dims = new_dims + list(coords[0].dims)\n\n    das = [xr.DataArray(it.numpy(), dims=dims, coords=co.coords)\n           for it, co in zip(items, coords)]\n\n    da_shape = dict(zip(coords[0].dims, self.da.shape[-len(coords[0].dims):]))\n    new_shape = dict(zip(new_dims, items[0].shape[:len(new_dims)]))\n\n    rec_da = xr.DataArray(\n        np.zeros([*new_shape.values(), *da_shape.values()]),\n        dims=dims,\n        coords={d: self.da[d] for d in self.patch_dims}\n    )\n    count_da = xr.zeros_like(rec_da)\n\n    for da in das:\n        rec_da.loc[da.coords] = rec_da.sel(da.coords) + da * w\n        count_da.loc[da.coords] = count_da.sel(da.coords) + w\n\n    return rec_da / count_da\n</code></pre>"},{"location":"pkg-doc/evaluate/","title":"evaluate","text":"<p>This module contains evaluation code for 4DVarNet models.</p> <p>It provides functions and utilities to compute evaluation metrics, compare model outputs with ground truth data, and generate diagnostic visualizations for model performance.</p>"},{"location":"pkg-doc/models/","title":"modeles","text":"<p>This module defines models and solvers for 4D-VarNet.</p> <p>4D-VarNet is a framework for solving inverse problems in data assimilation  using deep learning and PyTorch Lightning.</p> <p>Classes:</p> Name Description <code>Lit4dVarNet</code> <p>A PyTorch Lightning module for training and testing 4D-VarNet models.</p> <code>GradSolver</code> <p>A gradient-based solver for optimization in 4D-VarNet.</p> <code>ConvLstmGradModel</code> <p>A convolutional LSTM model for gradient modulation.</p> <code>BaseObsCost</code> <p>A base class for observation cost computation.</p> <code>BilinAEPriorCost</code> <p>A prior cost model using bilinear autoencoders.</p>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BaseObsCost","title":"<code>BaseObsCost</code>","text":"<p>               Bases: <code>Module</code></p> <p>A base class for computing observation cost.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>Weight for the observation cost.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>class BaseObsCost(nn.Module):\n    \"\"\"\n    A base class for computing observation cost.\n\n    Attributes:\n        w (float): Weight for the observation cost.\n    \"\"\"\n\n    def __init__(self, w=1) -&gt; None:\n        \"\"\"\n        Initialize the BaseObsCost module.\n\n        Args:\n            w (float, optional): Weight for the observation cost. Defaults to 1.\n        \"\"\"\n        super().__init__()\n        self.w = w\n\n    def forward(self, state, batch):\n        \"\"\"\n        Compute the observation cost.\n\n        Args:\n            state (torch.Tensor): The current state tensor.\n            batch (dict): The input batch containing data.\n\n        Returns:\n            torch.Tensor: The computed observation cost.\n        \"\"\"\n        msk = batch.input.isfinite()\n        return self.w * F.mse_loss(state[msk], batch.input.nan_to_num()[msk])\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BaseObsCost.__init__","title":"<code>__init__(w=1)</code>","text":"<p>Initialize the BaseObsCost module.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>float</code> <p>Weight for the observation cost. Defaults to 1.</p> <code>1</code> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def __init__(self, w=1) -&gt; None:\n    \"\"\"\n    Initialize the BaseObsCost module.\n\n    Args:\n        w (float, optional): Weight for the observation cost. Defaults to 1.\n    \"\"\"\n    super().__init__()\n    self.w = w\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BaseObsCost.forward","title":"<code>forward(state, batch)</code>","text":"<p>Compute the observation cost.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The current state tensor.</p> required <code>batch</code> <code>dict</code> <p>The input batch containing data.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed observation cost.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward(self, state, batch):\n    \"\"\"\n    Compute the observation cost.\n\n    Args:\n        state (torch.Tensor): The current state tensor.\n        batch (dict): The input batch containing data.\n\n    Returns:\n        torch.Tensor: The computed observation cost.\n    \"\"\"\n    msk = batch.input.isfinite()\n    return self.w * F.mse_loss(state[msk], batch.input.nan_to_num()[msk])\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BilinAEPriorCost","title":"<code>BilinAEPriorCost</code>","text":"<p>               Bases: <code>Module</code></p> <p>A prior cost model using bilinear autoencoders.</p> <p>Attributes:</p> Name Type Description <code>bilin_quad</code> <code>bool</code> <p>Whether to use bilinear quadratic terms.</p> <code>conv_in</code> <code>Conv2d</code> <p>Convolutional layer for input.</p> <code>conv_hidden</code> <code>Conv2d</code> <p>Convolutional layer for hidden states.</p> <code>bilin_1</code> <code>Conv2d</code> <p>Bilinear layer 1.</p> <code>bilin_21</code> <code>Conv2d</code> <p>Bilinear layer 2 (part 1).</p> <code>bilin_22</code> <code>Conv2d</code> <p>Bilinear layer 2 (part 2).</p> <code>conv_out</code> <code>Conv2d</code> <p>Convolutional layer for output.</p> <code>down</code> <code>Module</code> <p>Downsampling layer.</p> <code>up</code> <code>Module</code> <p>Upsampling layer.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>class BilinAEPriorCost(nn.Module):\n    \"\"\"\n    A prior cost model using bilinear autoencoders.\n\n    Attributes:\n        bilin_quad (bool): Whether to use bilinear quadratic terms.\n        conv_in (nn.Conv2d): Convolutional layer for input.\n        conv_hidden (nn.Conv2d): Convolutional layer for hidden states.\n        bilin_1 (nn.Conv2d): Bilinear layer 1.\n        bilin_21 (nn.Conv2d): Bilinear layer 2 (part 1).\n        bilin_22 (nn.Conv2d): Bilinear layer 2 (part 2).\n        conv_out (nn.Conv2d): Convolutional layer for output.\n        down (nn.Module): Downsampling layer.\n        up (nn.Module): Upsampling layer.\n    \"\"\"\n\n    def __init__(self, dim_in, dim_hidden, kernel_size=3, downsamp=None, bilin_quad=True):\n        \"\"\"\n        Initialize the BilinAEPriorCost module.\n\n        Args:\n            dim_in (int): Number of input dimensions.\n            dim_hidden (int): Number of hidden dimensions.\n            kernel_size (int, optional): Kernel size for convolutions. Defaults to 3.\n            downsamp (int, optional): Downsampling factor. Defaults to None.\n            bilin_quad (bool, optional): Whether to use bilinear quadratic terms. Defaults to True.\n        \"\"\"\n        super().__init__()\n        self.bilin_quad = bilin_quad\n        self.conv_in = nn.Conv2d(\n            dim_in, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n        self.conv_hidden = nn.Conv2d(\n            dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n\n        self.bilin_1 = nn.Conv2d(\n            dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n        self.bilin_21 = nn.Conv2d(\n            dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n        self.bilin_22 = nn.Conv2d(\n            dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n\n        self.conv_out = nn.Conv2d(\n            2 * dim_hidden, dim_in, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n\n        self.down = nn.AvgPool2d(downsamp) if downsamp is not None else nn.Identity()\n        self.up = (\n            nn.UpsamplingBilinear2d(scale_factor=downsamp)\n            if downsamp is not None\n            else nn.Identity()\n        )\n\n    def forward_ae(self, x):\n        \"\"\"\n        Perform the forward pass through the autoencoder.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after passing through the autoencoder.\n        \"\"\"\n        x = self.down(x)\n        x = self.conv_in(x)\n        x = self.conv_hidden(F.relu(x))\n\n        nonlin = (\n            self.bilin_21(x)**2\n            if self.bilin_quad\n            else (self.bilin_21(x) * self.bilin_22(x))\n        )\n        x = self.conv_out(\n            torch.cat([self.bilin_1(x), nonlin], dim=1)\n        )\n        x = self.up(x)\n        return x\n\n    def forward(self, state):\n        \"\"\"\n        Compute the prior cost using the autoencoder.\n\n        Args:\n            state (torch.Tensor): The current state tensor.\n\n        Returns:\n            torch.Tensor: The computed prior cost.\n        \"\"\"\n        return F.mse_loss(state, self.forward_ae(state))\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BilinAEPriorCost.__init__","title":"<code>__init__(dim_in, dim_hidden, kernel_size=3, downsamp=None, bilin_quad=True)</code>","text":"<p>Initialize the BilinAEPriorCost module.</p> <p>Parameters:</p> Name Type Description Default <code>dim_in</code> <code>int</code> <p>Number of input dimensions.</p> required <code>dim_hidden</code> <code>int</code> <p>Number of hidden dimensions.</p> required <code>kernel_size</code> <code>int</code> <p>Kernel size for convolutions. Defaults to 3.</p> <code>3</code> <code>downsamp</code> <code>int</code> <p>Downsampling factor. Defaults to None.</p> <code>None</code> <code>bilin_quad</code> <code>bool</code> <p>Whether to use bilinear quadratic terms. Defaults to True.</p> <code>True</code> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def __init__(self, dim_in, dim_hidden, kernel_size=3, downsamp=None, bilin_quad=True):\n    \"\"\"\n    Initialize the BilinAEPriorCost module.\n\n    Args:\n        dim_in (int): Number of input dimensions.\n        dim_hidden (int): Number of hidden dimensions.\n        kernel_size (int, optional): Kernel size for convolutions. Defaults to 3.\n        downsamp (int, optional): Downsampling factor. Defaults to None.\n        bilin_quad (bool, optional): Whether to use bilinear quadratic terms. Defaults to True.\n    \"\"\"\n    super().__init__()\n    self.bilin_quad = bilin_quad\n    self.conv_in = nn.Conv2d(\n        dim_in, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n    self.conv_hidden = nn.Conv2d(\n        dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n\n    self.bilin_1 = nn.Conv2d(\n        dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n    self.bilin_21 = nn.Conv2d(\n        dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n    self.bilin_22 = nn.Conv2d(\n        dim_hidden, dim_hidden, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n\n    self.conv_out = nn.Conv2d(\n        2 * dim_hidden, dim_in, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n\n    self.down = nn.AvgPool2d(downsamp) if downsamp is not None else nn.Identity()\n    self.up = (\n        nn.UpsamplingBilinear2d(scale_factor=downsamp)\n        if downsamp is not None\n        else nn.Identity()\n    )\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BilinAEPriorCost.forward","title":"<code>forward(state)</code>","text":"<p>Compute the prior cost using the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The current state tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed prior cost.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward(self, state):\n    \"\"\"\n    Compute the prior cost using the autoencoder.\n\n    Args:\n        state (torch.Tensor): The current state tensor.\n\n    Returns:\n        torch.Tensor: The computed prior cost.\n    \"\"\"\n    return F.mse_loss(state, self.forward_ae(state))\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.BilinAEPriorCost.forward_ae","title":"<code>forward_ae(x)</code>","text":"<p>Perform the forward pass through the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor after passing through the autoencoder.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward_ae(self, x):\n    \"\"\"\n    Perform the forward pass through the autoencoder.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after passing through the autoencoder.\n    \"\"\"\n    x = self.down(x)\n    x = self.conv_in(x)\n    x = self.conv_hidden(F.relu(x))\n\n    nonlin = (\n        self.bilin_21(x)**2\n        if self.bilin_quad\n        else (self.bilin_21(x) * self.bilin_22(x))\n    )\n    x = self.conv_out(\n        torch.cat([self.bilin_1(x), nonlin], dim=1)\n    )\n    x = self.up(x)\n    return x\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.ConvLstmGradModel","title":"<code>ConvLstmGradModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolutional LSTM model for gradient modulation.</p> <p>Attributes:</p> Name Type Description <code>dim_hidden</code> <code>int</code> <p>Number of hidden dimensions.</p> <code>gates</code> <code>Conv2d</code> <p>Convolutional gates for LSTM.</p> <code>conv_out</code> <code>Conv2d</code> <p>Output convolutional layer.</p> <code>dropout</code> <code>Dropout</code> <p>Dropout layer.</p> <code>down</code> <code>Module</code> <p>Downsampling layer.</p> <code>up</code> <code>Module</code> <p>Upsampling layer.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>class ConvLstmGradModel(nn.Module):\n    \"\"\"\n    A convolutional LSTM model for gradient modulation.\n\n    Attributes:\n        dim_hidden (int): Number of hidden dimensions.\n        gates (nn.Conv2d): Convolutional gates for LSTM.\n        conv_out (nn.Conv2d): Output convolutional layer.\n        dropout (nn.Dropout): Dropout layer.\n        down (nn.Module): Downsampling layer.\n        up (nn.Module): Upsampling layer.\n    \"\"\"\n\n    def __init__(self, dim_in, dim_hidden, kernel_size=3, dropout=0.1, downsamp=None):\n        \"\"\"\n        Initialize the ConvLstmGradModel.\n\n        Args:\n            dim_in (int): Number of input dimensions.\n            dim_hidden (int): Number of hidden dimensions.\n            kernel_size (int, optional): Kernel size for convolutions. Defaults to 3.\n            dropout (float, optional): Dropout rate. Defaults to 0.1.\n            downsamp (int, optional): Downsampling factor. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.dim_hidden = dim_hidden\n        self.gates = torch.nn.Conv2d(\n            dim_in + dim_hidden,\n            4 * dim_hidden,\n            kernel_size=kernel_size,\n            padding=kernel_size // 2,\n        )\n\n        self.conv_out = torch.nn.Conv2d(\n            dim_hidden, dim_in, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n\n        self.dropout = torch.nn.Dropout(dropout)\n        self._state = []\n        self.down = nn.AvgPool2d(downsamp) if downsamp is not None else nn.Identity()\n        self.up = (\n            nn.UpsamplingBilinear2d(scale_factor=downsamp)\n            if downsamp is not None\n            else nn.Identity()\n        )\n\n    def reset_state(self, inp):\n        \"\"\"\n        Reset the internal state of the LSTM.\n\n        Args:\n            inp (torch.Tensor): Input tensor to determine state size.\n        \"\"\"\n        size = [inp.shape[0], self.dim_hidden, *inp.shape[-2:]]\n        self._grad_norm = None\n        self._state = [\n            self.down(torch.zeros(size, device=inp.device)),\n            self.down(torch.zeros(size, device=inp.device)),\n        ]\n\n    def forward(self, x):\n        \"\"\"\n        Perform the forward pass of the LSTM.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        if self._grad_norm is None:\n            self._grad_norm = (x**2).mean().sqrt()\n        x = x / self._grad_norm\n        hidden, cell = self._state\n        x = self.dropout(x)\n        x = self.down(x)\n        gates = self.gates(torch.cat((x, hidden), 1))\n\n        in_gate, remember_gate, out_gate, cell_gate = gates.chunk(4, 1)\n\n        in_gate, remember_gate, out_gate = map(\n            torch.sigmoid, [in_gate, remember_gate, out_gate]\n        )\n        cell_gate = torch.tanh(cell_gate)\n\n        cell = (remember_gate * cell) + (in_gate * cell_gate)\n        hidden = out_gate * torch.tanh(cell)\n\n        self._state = hidden, cell\n        out = self.conv_out(hidden)\n        out = self.up(out)\n        return out\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.ConvLstmGradModel.__init__","title":"<code>__init__(dim_in, dim_hidden, kernel_size=3, dropout=0.1, downsamp=None)</code>","text":"<p>Initialize the ConvLstmGradModel.</p> <p>Parameters:</p> Name Type Description Default <code>dim_in</code> <code>int</code> <p>Number of input dimensions.</p> required <code>dim_hidden</code> <code>int</code> <p>Number of hidden dimensions.</p> required <code>kernel_size</code> <code>int</code> <p>Kernel size for convolutions. Defaults to 3.</p> <code>3</code> <code>dropout</code> <code>float</code> <p>Dropout rate. Defaults to 0.1.</p> <code>0.1</code> <code>downsamp</code> <code>int</code> <p>Downsampling factor. Defaults to None.</p> <code>None</code> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def __init__(self, dim_in, dim_hidden, kernel_size=3, dropout=0.1, downsamp=None):\n    \"\"\"\n    Initialize the ConvLstmGradModel.\n\n    Args:\n        dim_in (int): Number of input dimensions.\n        dim_hidden (int): Number of hidden dimensions.\n        kernel_size (int, optional): Kernel size for convolutions. Defaults to 3.\n        dropout (float, optional): Dropout rate. Defaults to 0.1.\n        downsamp (int, optional): Downsampling factor. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.dim_hidden = dim_hidden\n    self.gates = torch.nn.Conv2d(\n        dim_in + dim_hidden,\n        4 * dim_hidden,\n        kernel_size=kernel_size,\n        padding=kernel_size // 2,\n    )\n\n    self.conv_out = torch.nn.Conv2d(\n        dim_hidden, dim_in, kernel_size=kernel_size, padding=kernel_size // 2\n    )\n\n    self.dropout = torch.nn.Dropout(dropout)\n    self._state = []\n    self.down = nn.AvgPool2d(downsamp) if downsamp is not None else nn.Identity()\n    self.up = (\n        nn.UpsamplingBilinear2d(scale_factor=downsamp)\n        if downsamp is not None\n        else nn.Identity()\n    )\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.ConvLstmGradModel.forward","title":"<code>forward(x)</code>","text":"<p>Perform the forward pass of the LSTM.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Perform the forward pass of the LSTM.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    if self._grad_norm is None:\n        self._grad_norm = (x**2).mean().sqrt()\n    x = x / self._grad_norm\n    hidden, cell = self._state\n    x = self.dropout(x)\n    x = self.down(x)\n    gates = self.gates(torch.cat((x, hidden), 1))\n\n    in_gate, remember_gate, out_gate, cell_gate = gates.chunk(4, 1)\n\n    in_gate, remember_gate, out_gate = map(\n        torch.sigmoid, [in_gate, remember_gate, out_gate]\n    )\n    cell_gate = torch.tanh(cell_gate)\n\n    cell = (remember_gate * cell) + (in_gate * cell_gate)\n    hidden = out_gate * torch.tanh(cell)\n\n    self._state = hidden, cell\n    out = self.conv_out(hidden)\n    out = self.up(out)\n    return out\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.ConvLstmGradModel.reset_state","title":"<code>reset_state(inp)</code>","text":"<p>Reset the internal state of the LSTM.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>Tensor</code> <p>Input tensor to determine state size.</p> required Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def reset_state(self, inp):\n    \"\"\"\n    Reset the internal state of the LSTM.\n\n    Args:\n        inp (torch.Tensor): Input tensor to determine state size.\n    \"\"\"\n    size = [inp.shape[0], self.dim_hidden, *inp.shape[-2:]]\n    self._grad_norm = None\n    self._state = [\n        self.down(torch.zeros(size, device=inp.device)),\n        self.down(torch.zeros(size, device=inp.device)),\n    ]\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.GradSolver","title":"<code>GradSolver</code>","text":"<p>               Bases: <code>Module</code></p> <p>A gradient-based solver for optimization in 4D-VarNet.</p> <p>Attributes:</p> Name Type Description <code>prior_cost</code> <code>Module</code> <p>The prior cost function.</p> <code>obs_cost</code> <code>Module</code> <p>The observation cost function.</p> <code>grad_mod</code> <code>Module</code> <p>The gradient modulation model.</p> <code>n_step</code> <code>int</code> <p>Number of optimization steps.</p> <code>lr_grad</code> <code>float</code> <p>Learning rate for gradient updates.</p> <code>lbd</code> <code>float</code> <p>Regularization parameter.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>class GradSolver(nn.Module):\n    \"\"\"\n    A gradient-based solver for optimization in 4D-VarNet.\n\n    Attributes:\n        prior_cost (nn.Module): The prior cost function.\n        obs_cost (nn.Module): The observation cost function.\n        grad_mod (nn.Module): The gradient modulation model.\n        n_step (int): Number of optimization steps.\n        lr_grad (float): Learning rate for gradient updates.\n        lbd (float): Regularization parameter.\n    \"\"\"\n\n    def __init__(self, prior_cost, obs_cost, grad_mod, n_step, lr_grad=0.2, lbd=1.0, **kwargs):\n        \"\"\"\n        Initialize the GradSolver.\n\n        Args:\n            prior_cost (nn.Module): The prior cost function.\n            obs_cost (nn.Module): The observation cost function.\n            grad_mod (nn.Module): The gradient modulation model.\n            n_step (int): Number of optimization steps.\n            lr_grad (float, optional): Learning rate for gradient updates. Defaults to 0.2.\n            lbd (float, optional): Regularization parameter. Defaults to 1.0.\n        \"\"\"\n        super().__init__()\n        self.prior_cost = prior_cost\n        self.obs_cost = obs_cost\n        self.grad_mod = grad_mod\n\n        self.n_step = n_step\n        self.lr_grad = lr_grad\n        self.lbd = lbd\n\n        self._grad_norm = None\n\n    def init_state(self, batch, x_init=None):\n        \"\"\"\n        Initialize the state for optimization.\n\n        Args:\n            batch (dict): Input batch containing data.\n            x_init (torch.Tensor, optional): Initial state. Defaults to None.\n\n        Returns:\n            torch.Tensor: Initialized state.\n        \"\"\"\n        if x_init is not None:\n            return x_init\n\n        return batch.input.nan_to_num().detach().requires_grad_(True)\n\n    def solver_step(self, state, batch, step):\n        \"\"\"\n        Perform a single optimization step.\n\n        Args:\n            state (torch.Tensor): Current state.\n            batch (dict): Input batch containing data.\n            step (int): Current optimization step.\n\n        Returns:\n            torch.Tensor: Updated state.\n        \"\"\"\n        var_cost = self.prior_cost(state) + self.lbd**2 * self.obs_cost(state, batch)\n        grad = torch.autograd.grad(var_cost, state, create_graph=True)[0]\n\n        gmod = self.grad_mod(grad)\n        state_update = (\n            1 / (step + 1) * gmod\n            + self.lr_grad * (step + 1) / self.n_step * grad\n        )\n\n        return state - state_update\n\n    def forward(self, batch):\n        \"\"\"\n        Perform the forward pass of the solver.\n\n        Args:\n            batch (dict): Input batch containing data.\n\n        Returns:\n            torch.Tensor: Final optimized state.\n        \"\"\"\n        with torch.set_grad_enabled(True):\n            state = self.init_state(batch)\n            self.grad_mod.reset_state(batch.input)\n\n            for step in range(self.n_step):\n                state = self.solver_step(state, batch, step=step)\n                if not self.training:\n                    state = state.detach().requires_grad_(True)\n\n            if not self.training:\n                state = self.prior_cost.forward_ae(state)\n        return state\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.GradSolver.__init__","title":"<code>__init__(prior_cost, obs_cost, grad_mod, n_step, lr_grad=0.2, lbd=1.0, **kwargs)</code>","text":"<p>Initialize the GradSolver.</p> <p>Parameters:</p> Name Type Description Default <code>prior_cost</code> <code>Module</code> <p>The prior cost function.</p> required <code>obs_cost</code> <code>Module</code> <p>The observation cost function.</p> required <code>grad_mod</code> <code>Module</code> <p>The gradient modulation model.</p> required <code>n_step</code> <code>int</code> <p>Number of optimization steps.</p> required <code>lr_grad</code> <code>float</code> <p>Learning rate for gradient updates. Defaults to 0.2.</p> <code>0.2</code> <code>lbd</code> <code>float</code> <p>Regularization parameter. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def __init__(self, prior_cost, obs_cost, grad_mod, n_step, lr_grad=0.2, lbd=1.0, **kwargs):\n    \"\"\"\n    Initialize the GradSolver.\n\n    Args:\n        prior_cost (nn.Module): The prior cost function.\n        obs_cost (nn.Module): The observation cost function.\n        grad_mod (nn.Module): The gradient modulation model.\n        n_step (int): Number of optimization steps.\n        lr_grad (float, optional): Learning rate for gradient updates. Defaults to 0.2.\n        lbd (float, optional): Regularization parameter. Defaults to 1.0.\n    \"\"\"\n    super().__init__()\n    self.prior_cost = prior_cost\n    self.obs_cost = obs_cost\n    self.grad_mod = grad_mod\n\n    self.n_step = n_step\n    self.lr_grad = lr_grad\n    self.lbd = lbd\n\n    self._grad_norm = None\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.GradSolver.forward","title":"<code>forward(batch)</code>","text":"<p>Perform the forward pass of the solver.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch containing data.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Final optimized state.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward(self, batch):\n    \"\"\"\n    Perform the forward pass of the solver.\n\n    Args:\n        batch (dict): Input batch containing data.\n\n    Returns:\n        torch.Tensor: Final optimized state.\n    \"\"\"\n    with torch.set_grad_enabled(True):\n        state = self.init_state(batch)\n        self.grad_mod.reset_state(batch.input)\n\n        for step in range(self.n_step):\n            state = self.solver_step(state, batch, step=step)\n            if not self.training:\n                state = state.detach().requires_grad_(True)\n\n        if not self.training:\n            state = self.prior_cost.forward_ae(state)\n    return state\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.GradSolver.init_state","title":"<code>init_state(batch, x_init=None)</code>","text":"<p>Initialize the state for optimization.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch containing data.</p> required <code>x_init</code> <code>Tensor</code> <p>Initial state. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: Initialized state.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def init_state(self, batch, x_init=None):\n    \"\"\"\n    Initialize the state for optimization.\n\n    Args:\n        batch (dict): Input batch containing data.\n        x_init (torch.Tensor, optional): Initial state. Defaults to None.\n\n    Returns:\n        torch.Tensor: Initialized state.\n    \"\"\"\n    if x_init is not None:\n        return x_init\n\n    return batch.input.nan_to_num().detach().requires_grad_(True)\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.GradSolver.solver_step","title":"<code>solver_step(state, batch, step)</code>","text":"<p>Perform a single optimization step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>Current state.</p> required <code>batch</code> <code>dict</code> <p>Input batch containing data.</p> required <code>step</code> <code>int</code> <p>Current optimization step.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Updated state.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def solver_step(self, state, batch, step):\n    \"\"\"\n    Perform a single optimization step.\n\n    Args:\n        state (torch.Tensor): Current state.\n        batch (dict): Input batch containing data.\n        step (int): Current optimization step.\n\n    Returns:\n        torch.Tensor: Updated state.\n    \"\"\"\n    var_cost = self.prior_cost(state) + self.lbd**2 * self.obs_cost(state, batch)\n    grad = torch.autograd.grad(var_cost, state, create_graph=True)[0]\n\n    gmod = self.grad_mod(grad)\n    state_update = (\n        1 / (step + 1) * gmod\n        + self.lr_grad * (step + 1) / self.n_step * grad\n    )\n\n    return state - state_update\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet","title":"<code>Lit4dVarNet</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A PyTorch Lightning module for training and testing 4D-VarNet models.</p> <p>Attributes:</p> Name Type Description <code>solver</code> <code>GradSolver</code> <p>The solver used for optimization.</p> <code>rec_weight</code> <code>Tensor</code> <p>Reconstruction weight for loss computation.</p> <code>opt_fn</code> <code>callable</code> <p>Function to configure the optimizer.</p> <code>test_metrics</code> <code>dict</code> <p>Dictionary of test metrics.</p> <code>pre_metric_fn</code> <code>callable</code> <p>Preprocessing function for metrics.</p> <code>norm_stats</code> <code>tuple</code> <p>Normalization statistics (mean, std).</p> <code>persist_rw</code> <code>bool</code> <p>Whether to persist reconstruction weight as a buffer.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>class Lit4dVarNet(pl.LightningModule):\n    \"\"\"\n    A PyTorch Lightning module for training and testing 4D-VarNet models.\n\n    Attributes:\n        solver (GradSolver): The solver used for optimization.\n        rec_weight (torch.Tensor): Reconstruction weight for loss computation.\n        opt_fn (callable): Function to configure the optimizer.\n        test_metrics (dict): Dictionary of test metrics.\n        pre_metric_fn (callable): Preprocessing function for metrics.\n        norm_stats (tuple): Normalization statistics (mean, std).\n        persist_rw (bool): Whether to persist reconstruction weight as a buffer.\n    \"\"\"\n\n    def __init__(\n        self, solver, rec_weight, opt_fn, test_metrics=None,\n        pre_metric_fn=None, norm_stats=None, persist_rw=True\n    ):\n        \"\"\"\n        Initialize the Lit4dVarNet module.\n\n        Args:\n            solver (GradSolver): The solver used for optimization.\n            rec_weight (numpy.ndarray): Reconstruction weight for loss computation.\n            opt_fn (callable): Function to configure the optimizer.\n            test_metrics (dict, optional): Dictionary of test metrics.\n            pre_metric_fn (callable, optional): Preprocessing function for metrics.\n            norm_stats (tuple, optional): Normalization statistics (mean, std).\n            persist_rw (bool, optional): Whether to persist reconstruction weight as a buffer.\n        \"\"\"\n        super().__init__()\n        self.solver = solver\n        self.register_buffer('rec_weight', torch.from_numpy(rec_weight), persistent=persist_rw)\n        self.test_data = None\n        self._norm_stats = norm_stats\n        self.opt_fn = opt_fn\n        self.metrics = test_metrics or {}\n        self.pre_metric_fn = pre_metric_fn or (lambda x: x)\n\n    @property\n    def norm_stats(self):\n        \"\"\"\n        Retrieve normalization statistics (mean, std).\n\n        Returns:\n            tuple: Normalization statistics (mean, std).\n        \"\"\"\n        if self._norm_stats is not None:\n            return self._norm_stats\n        elif self.trainer.datamodule is not None:\n            return self.trainer.datamodule.norm_stats()\n        return (0., 1.)\n\n    @staticmethod\n    def weighted_mse(err, weight):\n        \"\"\"\n        Compute the weighted mean squared error.\n\n        Args:\n            err (torch.Tensor): Error tensor.\n            weight (torch.Tensor): Weight tensor.\n\n        Returns:\n            torch.Tensor: Weighted MSE loss.\n        \"\"\"\n        err_w = err * weight[None, ...]\n        non_zeros = (torch.ones_like(err) * weight[None, ...]) == 0.0\n        err_num = err.isfinite() &amp; ~non_zeros\n        if err_num.sum() == 0:\n            return torch.scalar_tensor(1000.0, device=err_num.device).requires_grad_()\n        loss = F.mse_loss(err_w[err_num], torch.zeros_like(err_w[err_num]))\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Perform a single training step.\n\n        Args:\n            batch (dict): Input batch.\n            batch_idx (int): Batch index.\n\n        Returns:\n            torch.Tensor: Training loss.\n        \"\"\"\n        return self.step(batch, \"train\")[0]\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Perform a single validation step.\n\n        Args:\n            batch (dict): Input batch.\n            batch_idx (int): Batch index.\n\n        Returns:\n            torch.Tensor: Validation loss.\n        \"\"\"\n        return self.step(batch, \"val\")[0]\n\n    def forward(self, batch):\n        \"\"\"\n        Forward pass through the solver.\n\n        Args:\n            batch (dict): Input batch.\n\n        Returns:\n            torch.Tensor: Solver output.\n        \"\"\"\n        return self.solver(batch)\n\n    def step(self, batch, phase=\"\"):\n        \"\"\"\n        Perform a single step for training or validation.\n\n        Args:\n            batch (dict): Input batch.\n            phase (str, optional): Phase (\"train\" or \"val\").\n\n        Returns:\n            tuple: Loss and output tensor.\n        \"\"\"\n        if self.training and batch.tgt.isfinite().float().mean() &lt; 0.9:\n            return None, None\n\n        loss, out = self.base_step(batch, phase)\n        grad_loss = self.weighted_mse(kfilts.sobel(out) - kfilts.sobel(batch.tgt), self.rec_weight)\n        prior_cost = self.solver.prior_cost(self.solver.init_state(batch, out))\n        self.log(f\"{phase}_gloss\", grad_loss, prog_bar=True, on_step=False, on_epoch=True)\n\n        training_loss = 50 * loss + 1000 * grad_loss + 1.0 * prior_cost\n        return training_loss, out\n\n    def base_step(self, batch, phase=\"\"):\n        \"\"\"\n        Perform the base step for loss computation.\n\n        Args:\n            batch (dict): Input batch.\n            phase (str, optional): Phase (\"train\" or \"val\").\n\n        Returns:\n            tuple: Loss and output tensor.\n        \"\"\"\n        out = self(batch=batch)\n        loss = self.weighted_mse(out - batch.tgt, self.rec_weight)\n\n        with torch.no_grad():\n            self.log(f\"{phase}_mse\", 10000 * loss * self.norm_stats[1]**2, prog_bar=True, on_step=False, on_epoch=True)\n            self.log(f\"{phase}_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n\n        return loss, out\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer.\n\n        Returns:\n            torch.optim.Optimizer: Optimizer instance.\n        \"\"\"\n        return self.opt_fn(self)\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Perform a single test step.\n\n        Args:\n            batch (dict): Input batch.\n            batch_idx (int): Batch index.\n        \"\"\"\n        if batch_idx == 0:\n            self.test_data = []\n        out = self(batch=batch)\n        m, s = self.norm_stats\n\n        self.test_data.append(torch.stack(\n            [\n                batch.input.cpu() * s + m,\n                batch.tgt.cpu() * s + m,\n                out.squeeze(dim=-1).detach().cpu() * s + m,\n            ],\n            dim=1,\n        ))\n\n    @property\n    def test_quantities(self):\n        \"\"\"\n        Retrieve the names of test quantities.\n\n        Returns:\n            list: List of test quantity names.\n        \"\"\"\n        return ['inp', 'tgt', 'out']\n\n    def on_test_epoch_end(self):\n        \"\"\"\n        Perform actions at the end of the test epoch.\n\n        This includes logging metrics and saving test data.\n        \"\"\"\n        rec_da = self.trainer.test_dataloaders.dataset.reconstruct(\n            self.test_data, self.rec_weight.cpu().numpy()\n        )\n\n        if isinstance(rec_da, list):\n            rec_da = rec_da[0]\n\n        self.test_data = rec_da.assign_coords(\n            dict(v0=self.test_quantities)\n        ).to_dataset(dim='v0')\n\n        metric_data = self.test_data.pipe(self.pre_metric_fn)\n        metrics = pd.Series({\n            metric_n: metric_fn(metric_data)\n            for metric_n, metric_fn in self.metrics.items()\n        })\n\n        print(metrics.to_frame(name=\"Metrics\").to_markdown())\n        if self.logger:\n            self.test_data.to_netcdf(Path(self.logger.log_dir) / 'test_data.nc')\n            print(Path(self.trainer.log_dir) / 'test_data.nc')\n            self.logger.log_metrics(metrics.to_dict())\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.norm_stats","title":"<code>norm_stats</code>  <code>property</code>","text":"<p>Retrieve normalization statistics (mean, std).</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Normalization statistics (mean, std).</p>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.test_quantities","title":"<code>test_quantities</code>  <code>property</code>","text":"<p>Retrieve the names of test quantities.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of test quantity names.</p>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.__init__","title":"<code>__init__(solver, rec_weight, opt_fn, test_metrics=None, pre_metric_fn=None, norm_stats=None, persist_rw=True)</code>","text":"<p>Initialize the Lit4dVarNet module.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>GradSolver</code> <p>The solver used for optimization.</p> required <code>rec_weight</code> <code>ndarray</code> <p>Reconstruction weight for loss computation.</p> required <code>opt_fn</code> <code>callable</code> <p>Function to configure the optimizer.</p> required <code>test_metrics</code> <code>dict</code> <p>Dictionary of test metrics.</p> <code>None</code> <code>pre_metric_fn</code> <code>callable</code> <p>Preprocessing function for metrics.</p> <code>None</code> <code>norm_stats</code> <code>tuple</code> <p>Normalization statistics (mean, std).</p> <code>None</code> <code>persist_rw</code> <code>bool</code> <p>Whether to persist reconstruction weight as a buffer.</p> <code>True</code> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def __init__(\n    self, solver, rec_weight, opt_fn, test_metrics=None,\n    pre_metric_fn=None, norm_stats=None, persist_rw=True\n):\n    \"\"\"\n    Initialize the Lit4dVarNet module.\n\n    Args:\n        solver (GradSolver): The solver used for optimization.\n        rec_weight (numpy.ndarray): Reconstruction weight for loss computation.\n        opt_fn (callable): Function to configure the optimizer.\n        test_metrics (dict, optional): Dictionary of test metrics.\n        pre_metric_fn (callable, optional): Preprocessing function for metrics.\n        norm_stats (tuple, optional): Normalization statistics (mean, std).\n        persist_rw (bool, optional): Whether to persist reconstruction weight as a buffer.\n    \"\"\"\n    super().__init__()\n    self.solver = solver\n    self.register_buffer('rec_weight', torch.from_numpy(rec_weight), persistent=persist_rw)\n    self.test_data = None\n    self._norm_stats = norm_stats\n    self.opt_fn = opt_fn\n    self.metrics = test_metrics or {}\n    self.pre_metric_fn = pre_metric_fn or (lambda x: x)\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.base_step","title":"<code>base_step(batch, phase='')</code>","text":"<p>Perform the base step for loss computation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <code>phase</code> <code>str</code> <p>Phase (\"train\" or \"val\").</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and output tensor.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def base_step(self, batch, phase=\"\"):\n    \"\"\"\n    Perform the base step for loss computation.\n\n    Args:\n        batch (dict): Input batch.\n        phase (str, optional): Phase (\"train\" or \"val\").\n\n    Returns:\n        tuple: Loss and output tensor.\n    \"\"\"\n    out = self(batch=batch)\n    loss = self.weighted_mse(out - batch.tgt, self.rec_weight)\n\n    with torch.no_grad():\n        self.log(f\"{phase}_mse\", 10000 * loss * self.norm_stats[1]**2, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(f\"{phase}_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n\n    return loss, out\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer.</p> <p>Returns:</p> Type Description <p>torch.optim.Optimizer: Optimizer instance.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"\n    Configure the optimizer.\n\n    Returns:\n        torch.optim.Optimizer: Optimizer instance.\n    \"\"\"\n    return self.opt_fn(self)\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.forward","title":"<code>forward(batch)</code>","text":"<p>Forward pass through the solver.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Solver output.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def forward(self, batch):\n    \"\"\"\n    Forward pass through the solver.\n\n    Args:\n        batch (dict): Input batch.\n\n    Returns:\n        torch.Tensor: Solver output.\n    \"\"\"\n    return self.solver(batch)\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.on_test_epoch_end","title":"<code>on_test_epoch_end()</code>","text":"<p>Perform actions at the end of the test epoch.</p> <p>This includes logging metrics and saving test data.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def on_test_epoch_end(self):\n    \"\"\"\n    Perform actions at the end of the test epoch.\n\n    This includes logging metrics and saving test data.\n    \"\"\"\n    rec_da = self.trainer.test_dataloaders.dataset.reconstruct(\n        self.test_data, self.rec_weight.cpu().numpy()\n    )\n\n    if isinstance(rec_da, list):\n        rec_da = rec_da[0]\n\n    self.test_data = rec_da.assign_coords(\n        dict(v0=self.test_quantities)\n    ).to_dataset(dim='v0')\n\n    metric_data = self.test_data.pipe(self.pre_metric_fn)\n    metrics = pd.Series({\n        metric_n: metric_fn(metric_data)\n        for metric_n, metric_fn in self.metrics.items()\n    })\n\n    print(metrics.to_frame(name=\"Metrics\").to_markdown())\n    if self.logger:\n        self.test_data.to_netcdf(Path(self.logger.log_dir) / 'test_data.nc')\n        print(Path(self.trainer.log_dir) / 'test_data.nc')\n        self.logger.log_metrics(metrics.to_dict())\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.step","title":"<code>step(batch, phase='')</code>","text":"<p>Perform a single step for training or validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <code>phase</code> <code>str</code> <p>Phase (\"train\" or \"val\").</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and output tensor.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def step(self, batch, phase=\"\"):\n    \"\"\"\n    Perform a single step for training or validation.\n\n    Args:\n        batch (dict): Input batch.\n        phase (str, optional): Phase (\"train\" or \"val\").\n\n    Returns:\n        tuple: Loss and output tensor.\n    \"\"\"\n    if self.training and batch.tgt.isfinite().float().mean() &lt; 0.9:\n        return None, None\n\n    loss, out = self.base_step(batch, phase)\n    grad_loss = self.weighted_mse(kfilts.sobel(out) - kfilts.sobel(batch.tgt), self.rec_weight)\n    prior_cost = self.solver.prior_cost(self.solver.init_state(batch, out))\n    self.log(f\"{phase}_gloss\", grad_loss, prog_bar=True, on_step=False, on_epoch=True)\n\n    training_loss = 50 * loss + 1000 * grad_loss + 1.0 * prior_cost\n    return training_loss, out\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Perform a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def test_step(self, batch, batch_idx):\n    \"\"\"\n    Perform a single test step.\n\n    Args:\n        batch (dict): Input batch.\n        batch_idx (int): Batch index.\n    \"\"\"\n    if batch_idx == 0:\n        self.test_data = []\n    out = self(batch=batch)\n    m, s = self.norm_stats\n\n    self.test_data.append(torch.stack(\n        [\n            batch.input.cpu() * s + m,\n            batch.tgt.cpu() * s + m,\n            out.squeeze(dim=-1).detach().cpu() * s + m,\n        ],\n        dim=1,\n    ))\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Training loss.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"\n    Perform a single training step.\n\n    Args:\n        batch (dict): Input batch.\n        batch_idx (int): Batch index.\n\n    Returns:\n        torch.Tensor: Training loss.\n    \"\"\"\n    return self.step(batch, \"train\")[0]\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Perform a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Validation loss.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"\n    Perform a single validation step.\n\n    Args:\n        batch (dict): Input batch.\n        batch_idx (int): Batch index.\n\n    Returns:\n        torch.Tensor: Validation loss.\n    \"\"\"\n    return self.step(batch, \"val\")[0]\n</code></pre>"},{"location":"pkg-doc/models/#ocean4dvarnet.models.Lit4dVarNet.weighted_mse","title":"<code>weighted_mse(err, weight)</code>  <code>staticmethod</code>","text":"<p>Compute the weighted mean squared error.</p> <p>Parameters:</p> Name Type Description Default <code>err</code> <code>Tensor</code> <p>Error tensor.</p> required <code>weight</code> <code>Tensor</code> <p>Weight tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Weighted MSE loss.</p> Source code in <code>ocean4dvarnet/models.py</code> <pre><code>@staticmethod\ndef weighted_mse(err, weight):\n    \"\"\"\n    Compute the weighted mean squared error.\n\n    Args:\n        err (torch.Tensor): Error tensor.\n        weight (torch.Tensor): Weight tensor.\n\n    Returns:\n        torch.Tensor: Weighted MSE loss.\n    \"\"\"\n    err_w = err * weight[None, ...]\n    non_zeros = (torch.ones_like(err) * weight[None, ...]) == 0.0\n    err_num = err.isfinite() &amp; ~non_zeros\n    if err_num.sum() == 0:\n        return torch.scalar_tensor(1000.0, device=err_num.device).requires_grad_()\n    loss = F.mse_loss(err_w[err_num], torch.zeros_like(err_w[err_num]))\n    return loss\n</code></pre>"},{"location":"pkg-doc/plot/","title":"plot","text":"<p>This module provides visualization utilities for 4DVarNet models.</p> <p>It includes functions for plotting model results, diagnostics, and comparisons between model outputs and ground truth data. These visualizations are designed to help analyze and interpret the performance of 4D-VarNet models.</p>"},{"location":"pkg-doc/process/","title":"process","text":"<p>This module provides post-processing utilities for 4DVarNet models.</p> <p>It includes functions for processing model outputs, applying transformations,  and preparing data for evaluation or visualization. These utilities are designed  to streamline the workflow after model inference.</p>"},{"location":"pkg-doc/train/","title":"train","text":"<p>This module provides training utilities for 4D-VarNet models using PyTorch Lightning.</p> <p>Functions:</p> Name Description <code>base_training</code> <p>Perform basic training and testing of a model with a single datamodule.</p> <code>multi_dm_training</code> <p>Perform training and testing with support for multiple datamodules.</p>"},{"location":"pkg-doc/train/#ocean4dvarnet.train.base_training","title":"<code>base_training(trainer, dm, lit_mod, ckpt=None)</code>","text":"<p>Perform basic training and testing of a model with a single datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer instance.</p> required <code>dm</code> <code>LightningDataModule</code> <p>The datamodule for training and testing.</p> required <code>lit_mod</code> <code>LightningModule</code> <p>The Lightning module to train.</p> required <code>ckpt</code> <code>str</code> <p>Path to a checkpoint to resume training from.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ocean4dvarnet/train.py</code> <pre><code>def base_training(trainer, dm, lit_mod, ckpt=None):\n    \"\"\"\n    Perform basic training and testing of a model with a single datamodule.\n\n    Args:\n        trainer (pl.Trainer): The PyTorch Lightning trainer instance.\n        dm (pl.LightningDataModule): The datamodule for training and testing.\n        lit_mod (pl.LightningModule): The Lightning module to train.\n        ckpt (str, optional): Path to a checkpoint to resume training from.\n\n    Returns:\n        None\n    \"\"\"\n    if trainer.logger is not None:\n        print()\n        print(\"Logdir:\", trainer.logger.log_dir)\n        print()\n\n    trainer.fit(lit_mod, datamodule=dm, ckpt_path=ckpt)\n    trainer.test(lit_mod, datamodule=dm, ckpt_path='best')\n</code></pre>"},{"location":"pkg-doc/train/#ocean4dvarnet.train.multi_dm_training","title":"<code>multi_dm_training(trainer, dm, lit_mod, test_dm=None, test_fn=None, ckpt=None)</code>","text":"<p>Perform training and testing with support for multiple datamodules.</p> <p>This function trains the model using the provided datamodule and optionally tests it on a separate test datamodule. It also supports custom test functions for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer instance.</p> required <code>dm</code> <code>LightningDataModule</code> <p>The datamodule for training.</p> required <code>lit_mod</code> <code>LightningModule</code> <p>The Lightning module to train.</p> required <code>test_dm</code> <code>LightningDataModule</code> <p>The datamodule for testing. Defaults to <code>dm</code>.</p> <code>None</code> <code>test_fn</code> <code>callable</code> <p>A custom function to evaluate the model after testing.</p> <code>None</code> <code>ckpt</code> <code>str</code> <p>Path to a checkpoint to resume training from.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ocean4dvarnet/train.py</code> <pre><code>def multi_dm_training(\n    trainer, dm, lit_mod, test_dm=None, test_fn=None, ckpt=None\n):\n    \"\"\"\n    Perform training and testing with support for multiple datamodules.\n\n    This function trains the model using the provided datamodule and optionally tests it\n    on a separate test datamodule. It also supports custom test functions for evaluation.\n\n    Args:\n        trainer (pl.Trainer): The PyTorch Lightning trainer instance.\n        dm (pl.LightningDataModule): The datamodule for training.\n        lit_mod (pl.LightningModule): The Lightning module to train.\n        test_dm (pl.LightningDataModule, optional): The datamodule for testing. Defaults to `dm`.\n        test_fn (callable, optional): A custom function to evaluate the model after testing.\n        ckpt (str, optional): Path to a checkpoint to resume training from.\n\n    Returns:\n        None\n    \"\"\"\n    if trainer.logger is not None:\n        print()\n        print(\"Logdir:\", trainer.logger.log_dir)\n        print()\n\n    trainer.fit(lit_mod, datamodule=dm, ckpt_path=ckpt)\n\n    if test_fn is not None:\n        if test_dm is None:\n            test_dm = dm\n        lit_mod._norm_stats = test_dm.norm_stats()\n\n        best_ckpt_path = trainer.checkpoint_callback.best_model_path\n        trainer.callbacks = []\n        trainer.test(lit_mod, datamodule=test_dm, ckpt_path=best_ckpt_path)\n\n        print(\"\\nBest ckpt score:\")\n        print(test_fn(lit_mod).to_markdown())\n        print(\"\\n###############\")\n</code></pre>"},{"location":"pkg-doc/utils/","title":"utils","text":"<p>This module provides utility functions for 4D-VarNet.</p> <p>Utility functions include data preprocessing, optimization configuration, diagnostics, and evaluation metrics.</p> <p>Functions:</p> Name Description <code>pipe</code> <p>Apply a sequence of functions to an input.</p> <code>kwgetattr</code> <p>Get an attribute of an object by name.</p> <code>callmap</code> <p>Apply a list of functions to an input and return the results.</p> <code>half_lr_adam</code> <p>Configure an Adam optimizer with specific learning rates for model components.</p> <code>cosanneal_lr_adam</code> <p>Configure an Adam optimizer with cosine annealing learning rate scheduling.</p> <code>cosanneal_lr_lion</code> <p>Configure a Lion optimizer with cosine annealing learning rate scheduling.</p> <code>triang_lr_adam</code> <p>Configure an Adam optimizer with triangular cyclic learning rate scheduling.</p> <code>remove_nan</code> <p>Fill NaN values in a DataArray using Gauss-Seidel interpolation.</p> <code>get_constant_crop</code> <p>Generate a constant cropping mask for patches.</p> <code>get_cropped_hanning_mask</code> <p>Generate a cropped Hanning mask for patches.</p> <code>get_triang_time_wei</code> <p>Generate a triangular time weighting mask for patches.</p> <code>load_enatl</code> <p>Load ENATL dataset and preprocess it.</p> <code>load_altimetry_data</code> <p>Load altimetry data and preprocess it.</p> <code>load_dc_data</code> <p>Load DC data (currently a placeholder function).</p> <code>load_full_natl_data</code> <p>Load full NATL dataset and preprocess it.</p> <code>rmse_based_scores_from_ds</code> <p>Compute RMSE-based scores from a dataset.</p> <code>psd_based_scores_from_ds</code> <p>Compute PSD-based scores from a dataset.</p> <code>rmse_based_scores</code> <p>Compute RMSE-based scores for reconstruction evaluation.</p> <code>psd_based_scores</code> <p>Compute PSD-based scores for reconstruction evaluation.</p> <code>diagnostics</code> <p>Compute diagnostics for a given test domain.</p> <code>diagnostics_from_ds</code> <p>Compute diagnostics from a dataset.</p> <code>test_osse</code> <p>Perform OSSE testing and compute metrics.</p> <code>ensemble_metrics</code> <p>Compute ensemble metrics for multiple checkpoints.</p> <code>add_geo_attrs</code> <p>Add geographic attributes to a DataArray.</p> <code>vort</code> <p>Compute vorticity from a DataArray.</p> <code>geo_energy</code> <p>Compute geostrophic energy from a DataArray.</p> <code>best_ckpt</code> <p>Retrieve the best checkpoint from an experiment directory.</p> <code>load_cfg</code> <p>Load configuration files for an experiment.</p>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.add_geo_attrs","title":"<code>add_geo_attrs(da)</code>","text":"<p>Add geographic attributes (longitude and latitude units) to a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <p>Returns:</p> Type Description <p>xarray.DataArray: The DataArray with geographic attributes added.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def add_geo_attrs(da):\n    \"\"\"\n    Add geographic attributes (longitude and latitude units) to a DataArray.\n\n    Args:\n        da (xarray.DataArray): The input DataArray.\n\n    Returns:\n        xarray.DataArray: The DataArray with geographic attributes added.\n    \"\"\"\n    da[\"lon\"] = da.lon.assign_attrs(units=\"degrees_east\")\n    da[\"lat\"] = da.lat.assign_attrs(units=\"degrees_north\")\n    return da\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.best_ckpt","title":"<code>best_ckpt(xp_dir)</code>","text":"<p>Retrieve the best checkpoint from an experiment directory.</p> <p>Parameters:</p> Name Type Description Default <code>xp_dir</code> <code>str</code> <p>Path to the experiment directory.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the best checkpoint file.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def best_ckpt(xp_dir):\n    \"\"\"\n    Retrieve the best checkpoint from an experiment directory.\n\n    Args:\n        xp_dir (str): Path to the experiment directory.\n\n    Returns:\n        str: Path to the best checkpoint file.\n    \"\"\"\n    _, xpn = load_cfg(xp_dir)\n    if xpn is None:\n        return None\n    print(Path(xp_dir) / xpn / 'checkpoints')\n    ckpt_last = max(\n        (Path(xp_dir) / xpn / 'checkpoints').glob(\"*.ckpt\"), key=lambda p: p.stat().st_mtime\n    )\n    cbs = torch.load(ckpt_last)[\"callbacks\"]\n    ckpt_cb = cbs[next(k for k in cbs.keys() if \"ModelCheckpoint\" in k)]\n    return ckpt_cb[\"best_model_path\"]\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.callmap","title":"<code>callmap(inp, fns)</code>","text":"<p>Apply a list of functions to an input and return the results.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <p>The input to process.</p> required <code>fns</code> <code>list</code> <p>A list of functions to apply.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of results from applying each function.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def callmap(inp, fns):\n    \"\"\"\n    Apply a list of functions to an input and return the results.\n\n    Args:\n        inp: The input to process.\n        fns (list): A list of functions to apply.\n\n    Returns:\n        list: A list of results from applying each function.\n    \"\"\"\n    return [fn(inp) for fn in fns]\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.cosanneal_lr_adam","title":"<code>cosanneal_lr_adam(lit_mod, lr, T_max=100, weight_decay=0.0)</code>","text":"<p>Configure an Adam optimizer with cosine annealing learning rate scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>lit_mod</code> <p>The Lightning module containing the model.</p> required <code>lr</code> <code>float</code> <p>The base learning rate.</p> required <code>T_max</code> <code>int</code> <p>Maximum number of iterations for the scheduler.</p> <code>100</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for the optimizer.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def cosanneal_lr_adam(lit_mod, lr, T_max=100, weight_decay=0.):\n    \"\"\"\n    Configure an Adam optimizer with cosine annealing learning rate scheduling.\n\n    Args:\n        lit_mod: The Lightning module containing the model.\n        lr (float): The base learning rate.\n        T_max (int): Maximum number of iterations for the scheduler.\n        weight_decay (float): Weight decay for the optimizer.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n    \"\"\"\n    opt = torch.optim.Adam(\n        [\n            {\"params\": lit_mod.solver.grad_mod.parameters(), \"lr\": lr},\n            {\"params\": lit_mod.solver.obs_cost.parameters(), \"lr\": lr},\n            {\"params\": lit_mod.solver.prior_cost.parameters(), \"lr\": lr / 2},\n        ], weight_decay=weight_decay\n    )\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt, T_max=T_max\n        ),\n    }\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.cosanneal_lr_lion","title":"<code>cosanneal_lr_lion(lit_mod, lr, T_max=100)</code>","text":"<p>Configure a Lion optimizer with cosine annealing learning rate scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>lit_mod</code> <p>The Lightning module containing the model.</p> required <code>lr</code> <code>float</code> <p>The base learning rate.</p> required <code>T_max</code> <code>int</code> <p>Maximum number of iterations for the scheduler.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def cosanneal_lr_lion(lit_mod, lr, T_max=100):\n    \"\"\"\n    Configure a Lion optimizer with cosine annealing learning rate scheduling.\n\n    Args:\n        lit_mod: The Lightning module containing the model.\n        lr (float): The base learning rate.\n        T_max (int): Maximum number of iterations for the scheduler.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n    \"\"\"\n    import lion_pytorch\n    opt = lion_pytorch.Lion(\n        [\n            {\"params\": lit_mod.solver.grad_mod.parameters(), \"lr\": lr},\n            {\"params\": lit_mod.solver.prior_cost.parameters(), \"lr\": lr / 2},\n        ], weight_decay=1e-3\n    )\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=T_max),\n    }\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.diagnostics","title":"<code>diagnostics(lit_mod, test_domain)</code>","text":"<p>Compute diagnostics for a given test domain.</p> <p>Parameters:</p> Name Type Description Default <code>lit_mod</code> <p>The Lightning module containing the model.</p> required <code>test_domain</code> <code>dict</code> <p>The test domain to evaluate.</p> required <p>Returns:</p> Type Description <p>pandas.Series: A series containing diagnostic metrics.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def diagnostics(lit_mod, test_domain):\n    \"\"\"\n    Compute diagnostics for a given test domain.\n\n    Args:\n        lit_mod: The Lightning module containing the model.\n        test_domain (dict): The test domain to evaluate.\n\n    Returns:\n        pandas.Series: A series containing diagnostic metrics.\n    \"\"\"\n    test_data = lit_mod.test_data.sel(test_domain)\n    return diagnostics_from_ds(test_data, test_domain)\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.diagnostics_from_ds","title":"<code>diagnostics_from_ds(test_data, test_domain)</code>","text":"<p>Compute diagnostics from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>test_data</code> <code>Dataset</code> <p>The test data.</p> required <code>test_domain</code> <code>dict</code> <p>The test domain to evaluate.</p> required <p>Returns:</p> Type Description <p>pandas.Series: A series containing diagnostic metrics.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def diagnostics_from_ds(test_data, test_domain):\n    \"\"\"\n    Compute diagnostics from a dataset.\n\n    Args:\n        test_data (xarray.Dataset): The test data.\n        test_domain (dict): The test domain to evaluate.\n\n    Returns:\n        pandas.Series: A series containing diagnostic metrics.\n    \"\"\"\n    test_data = test_data.sel(test_domain)\n    metrics = {\n        \"RMSE (m)\": test_data.pipe(lambda ds: (ds.out - ds.tgt))\n        .pipe(lambda da: da**2)\n        .mean()\n        .pipe(np.sqrt)\n        .item(),\n        **dict(\n            zip(\n                [\"\u03bbx\", \"\u03bbt\"],\n                test_data.pipe(lambda ds: psd_based_scores(ds.out, ds.tgt)[1:]),\n            )\n        ),\n        **dict(\n            zip(\n                [\"\u03bc\", \"\u03c3\"],\n                test_data.pipe(lambda ds: rmse_based_scores(ds.out, ds.tgt)[2:]),\n            )\n        ),\n    }\n    return pd.Series(metrics, name=\"osse_metrics\")\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.ensemble_metrics","title":"<code>ensemble_metrics(trainer, lit_mod, ckpt_list, dm, save_path)</code>","text":"<p>Compute ensemble metrics for multiple checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer instance.</p> required <code>lit_mod</code> <code>LightningModule</code> <p>The Lightning module to test.</p> required <code>ckpt_list</code> <code>list</code> <p>List of checkpoint paths to evaluate.</p> required <code>dm</code> <code>LightningDataModule</code> <p>The datamodule for testing.</p> required <code>save_path</code> <code>str</code> <p>Path to save the metrics and ensemble outputs.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def ensemble_metrics(trainer, lit_mod, ckpt_list, dm, save_path):\n    \"\"\"\n    Compute ensemble metrics for multiple checkpoints.\n\n    Args:\n        trainer (pl.Trainer): The PyTorch Lightning trainer instance.\n        lit_mod (pl.LightningModule): The Lightning module to test.\n        ckpt_list (list): List of checkpoint paths to evaluate.\n        dm (pl.LightningDataModule): The datamodule for testing.\n        save_path (str): Path to save the metrics and ensemble outputs.\n\n    Returns:\n        None\n    \"\"\"\n    metrics = []\n    test_data = xr.Dataset()\n    for i, ckpt in enumerate(ckpt_list):\n        trainer.test(lit_mod, ckpt_path=ckpt, datamodule=dm)\n        rmse = (\n            lit_mod.test_data.pipe(lambda ds: (ds.out - ds.ssh))\n            .pipe(lambda da: da**2)\n            .mean()\n            .pipe(np.sqrt)\n            .item()\n        )\n        lx, lt = psd_based_scores(lit_mod.test_data.out, lit_mod.test_data.ssh)[1:]\n        mu, sig = rmse_based_scores(lit_mod.test_data.out, lit_mod.test_data.ssh)[2:]\n\n        metrics.append(dict(ckpt=ckpt, rmse=rmse, lx=lx, lt=lt, mu=mu, sig=sig))\n\n        if i == 0:\n            test_data = lit_mod.test_data\n            test_data = test_data.rename(out=f\"out_{i}\")\n        else:\n            test_data = test_data.assign(**{f\"out_{i}\": lit_mod.test_data.out})\n        test_data[f\"out_{i}\"] = test_data[f\"out_{i}\"].assign_attrs(\n            ckpt=str(ckpt)\n        )\n\n    metric_df = pd.DataFrame(metrics)\n    print(metric_df.to_markdown())\n    print(metric_df.describe().to_markdown())\n    metric_df.to_csv(save_path + \"/metrics.csv\")\n    test_data.to_netcdf(save_path + \"ens_out.nc\")\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.geo_energy","title":"<code>geo_energy(da)</code>","text":"<p>Compute the geostrophic energy from a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <p>Returns:</p> Type Description <p>xarray.DataArray: The geostrophic energy computed from the input data.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def geo_energy(da):\n    \"\"\"\n    Compute the geostrophic energy from a DataArray.\n\n    Args:\n        da (xarray.DataArray): The input DataArray.\n\n    Returns:\n        xarray.DataArray: The geostrophic energy computed from the input data.\n    \"\"\"\n    return np.hypot(*mpcalc.geostrophic_wind(da.pipe(add_geo_attrs))).metpy.dequantify()\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.get_constant_crop","title":"<code>get_constant_crop(patch_dims, crop, dim_order=['time', 'lat', 'lon'])</code>","text":"<p>Generate a constant cropping mask for patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch_dims</code> <code>dict</code> <p>Dimensions of the patch.</p> required <code>crop</code> <code>dict</code> <p>Crop sizes for each dimension.</p> required <code>dim_order</code> <code>list</code> <p>Order of dimensions.</p> <code>['time', 'lat', 'lon']</code> <p>Returns:</p> Type Description <p>numpy.ndarray: A mask with cropped regions set to 0 and others to 1.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def get_constant_crop(patch_dims, crop, dim_order=[\"time\", \"lat\", \"lon\"]):\n    \"\"\"\n    Generate a constant cropping mask for patches.\n\n    Args:\n        patch_dims (dict): Dimensions of the patch.\n        crop (dict): Crop sizes for each dimension.\n        dim_order (list): Order of dimensions.\n\n    Returns:\n        numpy.ndarray: A mask with cropped regions set to 0 and others to 1.\n    \"\"\"\n    patch_weight = np.zeros([patch_dims[d] for d in dim_order], dtype=\"float32\")\n    mask = tuple(\n        slice(crop[d], -crop[d]) if crop.get(d, 0) &gt; 0 else slice(None, None)\n        for d in dim_order\n    )\n    patch_weight[mask] = 1.0\n    return patch_weight\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.get_cropped_hanning_mask","title":"<code>get_cropped_hanning_mask(patch_dims, crop, **kwargs)</code>","text":"<p>Generate a cropped Hanning mask for patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch_dims</code> <code>dict</code> <p>Dimensions of the patch.</p> required <code>crop</code> <code>dict</code> <p>Crop sizes for each dimension.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The cropped Hanning mask.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def get_cropped_hanning_mask(patch_dims, crop, **kwargs):\n    \"\"\"\n    Generate a cropped Hanning mask for patches.\n\n    Args:\n        patch_dims (dict): Dimensions of the patch.\n        crop (dict): Crop sizes for each dimension.\n\n    Returns:\n        numpy.ndarray: The cropped Hanning mask.\n    \"\"\"\n    pw = get_constant_crop(patch_dims, crop)\n    t_msk = kornia.filters.get_hanning_kernel1d(patch_dims[\"time\"])\n    patch_weight = t_msk[:, None, None] * pw\n    return patch_weight.cpu().numpy()\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.get_triang_time_wei","title":"<code>get_triang_time_wei(patch_dims, offset=0, **crop_kw)</code>","text":"<p>Generate a triangular time weighting mask for patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch_dims</code> <code>dict</code> <p>Dimensions of the patch.</p> required <code>offset</code> <code>int</code> <p>Offset for the triangular weighting.</p> <code>0</code> <code>crop_kw</code> <code>dict</code> <p>Additional cropping parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <p>numpy.ndarray: The triangular time weighting mask.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def get_triang_time_wei(patch_dims, offset=0, **crop_kw):\n    \"\"\"\n    Generate a triangular time weighting mask for patches.\n\n    Args:\n        patch_dims (dict): Dimensions of the patch.\n        offset (int): Offset for the triangular weighting.\n        crop_kw (dict): Additional cropping parameters.\n\n    Returns:\n        numpy.ndarray: The triangular time weighting mask.\n    \"\"\"\n    pw = get_constant_crop(patch_dims, **crop_kw)\n    return np.fromfunction(\n        lambda t, *a: (\n            (1 - np.abs(offset + 2 * t - patch_dims[\"time\"]) / patch_dims[\"time\"]) * pw\n        ),\n        patch_dims.values(),\n    )\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.half_lr_adam","title":"<code>half_lr_adam(lit_mod, lr)</code>","text":"<p>Configure an Adam optimizer with specific learning rates for model components.</p> <p>Parameters:</p> Name Type Description Default <code>lit_mod</code> <p>The Lightning module containing the model.</p> required <code>lr</code> <code>float</code> <p>The base learning rate.</p> required <p>Returns:</p> Type Description <p>torch.optim.Adam: The configured optimizer.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def half_lr_adam(lit_mod, lr):\n    \"\"\"\n    Configure an Adam optimizer with specific learning rates for model components.\n\n    Args:\n        lit_mod: The Lightning module containing the model.\n        lr (float): The base learning rate.\n\n    Returns:\n        torch.optim.Adam: The configured optimizer.\n    \"\"\"\n    return torch.optim.Adam(\n        [\n            {\"params\": lit_mod.solver.grad_mod.parameters(), \"lr\": lr},\n            {\"params\": lit_mod.solver.obs_cost.parameters(), \"lr\": lr},\n            {\"params\": lit_mod.solver.prior_cost.parameters(), \"lr\": lr / 2},\n        ],\n    )\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.kwgetattr","title":"<code>kwgetattr(obj, name)</code>","text":"<p>Get an attribute of an object by name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The object to query.</p> required <code>name</code> <code>str</code> <p>The name of the attribute.</p> required <p>Returns:</p> Type Description <p>The value of the attribute.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def kwgetattr(obj, name):\n    \"\"\"\n    Get an attribute of an object by name.\n\n    Args:\n        obj: The object to query.\n        name (str): The name of the attribute.\n\n    Returns:\n        The value of the attribute.\n    \"\"\"\n    return getattr(obj, name)\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.load_altimetry_data","title":"<code>load_altimetry_data(path, obs_from_tgt=False)</code>","text":"<p>Load and preprocess altimetry data.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the altimetry dataset.</p> required <code>obs_from_tgt</code> <code>bool</code> <p>Whether to use target data as observations.</p> <code>False</code> <p>Returns:</p> Type Description <p>xarray.DataArray: The preprocessed altimetry dataset.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def load_altimetry_data(path, obs_from_tgt=False):\n    \"\"\"\n    Load and preprocess altimetry data.\n\n    Args:\n        path (str): Path to the altimetry dataset.\n        obs_from_tgt (bool): Whether to use target data as observations.\n\n    Returns:\n        xarray.DataArray: The preprocessed altimetry dataset.\n    \"\"\"\n    ds = (\n        xr.open_dataset(path)\n        # .assign(ssh=lambda ds: ds.ssh.coarsen(lon=2, lat=2).mean().interp(lat=ds.lat, lon=ds.lon))\n        .load()\n        .assign(\n            input=lambda ds: ds.nadir_obs,\n            tgt=lambda ds: remove_nan(ds.ssh),\n        )\n    )\n\n    if obs_from_tgt:\n        ds = ds.assign(input=ds.tgt.where(np.isfinite(ds.input), np.nan))\n\n    return (\n        ds[[*data.TrainingItem._fields]]\n        .transpose(\"time\", \"lat\", \"lon\")\n        .to_array()\n    )\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.load_cfg","title":"<code>load_cfg(xp_dir)</code>","text":"<p>Load configuration files for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>xp_dir</code> <code>str</code> <p>Path to the experiment directory.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the configuration and the experiment name.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def load_cfg(xp_dir):\n    \"\"\"\n    Load configuration files for an experiment.\n\n    Args:\n        xp_dir (str): Path to the experiment directory.\n\n    Returns:\n        tuple: A tuple containing the configuration and the experiment name.\n    \"\"\"\n    hydra_cfg = OmegaConf.load(Path(xp_dir) / \".hydra/hydra.yaml\").hydra\n    cfg = OmegaConf.load(Path(xp_dir) / \".hydra/config.yaml\")\n    OmegaConf.register_new_resolver(\n        \"hydra\", lambda k: OmegaConf.select(hydra_cfg, k), replace=True\n    )\n    try:\n        OmegaConf.resolve(cfg)\n        OmegaConf.resolve(cfg)\n    except Exception:\n        return None, None\n\n    return cfg, OmegaConf.select(hydra_cfg, \"runtime.choices.xp\")\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.load_dc_data","title":"<code>load_dc_data(**kwargs)</code>","text":"<p>Load DC data.</p> <p>This is currently a placeholder function for loading DC data.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def load_dc_data(**kwargs):\n    \"\"\"\n    Load DC data.\n\n    This is currently a placeholder function for loading DC data.\n\n    Args:\n        kwargs\n\n    Returns:\n        None\n    \"\"\"\n    path_gt = \"../sla-data-registry/NATL60/NATL/ref_new/NATL60-CJM165_NATL_ssh_y2013.1y.nc\",\n    path_obs = \"NATL60/NATL/data_new/dataset_nadir_0d.nc\"\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.load_enatl","title":"<code>load_enatl(*args, obs_from_tgt=True, **kwargs)</code>","text":"<p>Load and preprocess the ENATL dataset.</p> <p>Parameters:</p> Name Type Description Default <code>obs_from_tgt</code> <code>bool</code> <p>Whether to use target data as observations.</p> <code>True</code> <p>Returns:</p> Type Description <p>xarray.DataArray: The preprocessed ENATL dataset.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def load_enatl(*args, obs_from_tgt=True, **kwargs):\n    \"\"\"\n    Load and preprocess the ENATL dataset.\n\n    Args:\n        obs_from_tgt (bool): Whether to use target data as observations.\n\n    Returns:\n        xarray.DataArray: The preprocessed ENATL dataset.\n    \"\"\"\n    # ds = xr.open_dataset('../sla-data-registry/qdata/enatl_wo_tide.nc')\n    # print(ds)\n    # return ds.rename(nadir_obs='input', ssh='tgt')\\\n    #     .to_array()\\\n    #     .transpose('variable', 'time', 'lat', 'lon')\\\n    #     .sortby('variable')\n    ssh = xr.open_zarr('../sla-data-registry/enatl_preproc/truth_SLA_SSH_NATL60.zarr/').ssh\n    nadirs = xr.open_zarr('../sla-data-registry/enatl_preproc/SLA_SSH_5nadirs.zarr/').ssh\n    ssh = ssh.interp(\n        lon=np.arange(ssh.lon.min(), ssh.lon.max(), 1/20),\n        lat=np.arange(ssh.lat.min(), ssh.lat.max(), 1/20)\n    )\n    nadirs = nadirs.interp(time=ssh.time, method='nearest')\\\n        .interp(lat=ssh.lat, lon=ssh.lon, method='zero')\n    ds = xr.Dataset(dict(input=nadirs, tgt=(ssh.dims, ssh.values)), nadirs.coords)\n    if obs_from_tgt:\n        ds = ds.assign(input=ds.tgt.transpose(*ds.input.dims).where(np.isfinite(ds.input), np.nan))\n    return ds.transpose('time', 'lat', 'lon').to_array().load().sortby('variable')\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.load_full_natl_data","title":"<code>load_full_natl_data(path_obs='../sla-data-registry/CalData/cal_data_new_errs.nc', path_gt='../sla-data-registry/NATL60/NATL/ref_new/NATL60-CJM165_NATL_ssh_y2013.1y.nc', obs_var='five_nadirs', gt_var='ssh', **kwargs)</code>","text":"<p>Load and preprocess the full NATL dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path_obs</code> <code>str</code> <p>Path to the observation dataset.</p> <code>'../sla-data-registry/CalData/cal_data_new_errs.nc'</code> <code>path_gt</code> <code>str</code> <p>Path to the ground truth dataset.</p> <code>'../sla-data-registry/NATL60/NATL/ref_new/NATL60-CJM165_NATL_ssh_y2013.1y.nc'</code> <code>obs_var</code> <code>str</code> <p>Observation variable name.</p> <code>'five_nadirs'</code> <code>gt_var</code> <code>str</code> <p>Ground truth variable name.</p> <code>'ssh'</code> <p>Returns:</p> Type Description <p>xarray.DataArray: The preprocessed NATL dataset.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def load_full_natl_data(\n    path_obs=\"../sla-data-registry/CalData/cal_data_new_errs.nc\",\n    path_gt=\"../sla-data-registry/NATL60/NATL/ref_new/NATL60-CJM165_NATL_ssh_y2013.1y.nc\",\n    obs_var='five_nadirs',\n    gt_var='ssh',\n    **kwargs\n):\n    \"\"\"\n    Load and preprocess the full NATL dataset.\n\n    Args:\n        path_obs (str): Path to the observation dataset.\n        path_gt (str): Path to the ground truth dataset.\n        obs_var (str): Observation variable name.\n        gt_var (str): Ground truth variable name.\n\n    Returns:\n        xarray.DataArray: The preprocessed NATL dataset.\n    \"\"\"\n    inp = xr.open_dataset(path_obs)[obs_var]\n    gt = (\n        xr.open_dataset(path_gt)[gt_var]\n        # .isel(time=slice(0, -1))\n        .sel(lat=inp.lat, lon=inp.lon, method=\"nearest\")\n    )\n\n    return xr.Dataset(dict(input=inp, tgt=(gt.dims, gt.values)), inp.coords).to_array().sortby('variable')\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.pipe","title":"<code>pipe(inp, fns)</code>","text":"<p>Apply a sequence of functions to an input.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <p>The input to process.</p> required <code>fns</code> <code>list</code> <p>A list of functions to apply.</p> required <p>Returns:</p> Type Description <p>The processed input after applying all functions.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def pipe(inp, fns):\n    \"\"\"\n    Apply a sequence of functions to an input.\n\n    Args:\n        inp: The input to process.\n        fns (list): A list of functions to apply.\n\n    Returns:\n        The processed input after applying all functions.\n    \"\"\"\n    for f in fns:\n        inp = f(inp)\n    return inp\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.psd_based_scores","title":"<code>psd_based_scores(da_rec, da_ref)</code>","text":"<p>Compute PSD-based scores for reconstruction evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>da_rec</code> <code>DataArray</code> <p>The reconstructed data.</p> required <code>da_ref</code> <code>DataArray</code> <p>The reference data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing PSD-based scores and resolved wavelengths.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def psd_based_scores(da_rec, da_ref):\n    \"\"\"\n    Compute PSD-based scores for reconstruction evaluation.\n\n    Args:\n        da_rec (xarray.DataArray): The reconstructed data.\n        da_ref (xarray.DataArray): The reference data.\n\n    Returns:\n        tuple: A tuple containing PSD-based scores and resolved wavelengths.\n    \"\"\"\n    err = da_rec - da_ref\n    err[\"time\"] = (err.time - err.time[0]) / np.timedelta64(1, \"D\")\n    signal = da_ref\n    signal[\"time\"] = (signal.time - signal.time[0]) / np.timedelta64(1, \"D\")\n    psd_err = xrft.power_spectrum(\n        err, dim=[\"time\", \"lon\"], detrend=\"constant\", window=\"hann\"\n    ).compute()\n    psd_signal = xrft.power_spectrum(\n        signal, dim=[\"time\", \"lon\"], detrend=\"constant\", window=\"hann\"\n    ).compute()\n    mean_psd_signal = psd_signal.mean(dim=\"lat\").where(\n        (psd_signal.freq_lon &gt; 0.0) &amp; (psd_signal.freq_time &gt; 0), drop=True\n    )\n    mean_psd_err = psd_err.mean(dim=\"lat\").where(\n        (psd_err.freq_lon &gt; 0.0) &amp; (psd_err.freq_time &gt; 0), drop=True\n    )\n    psd_based_score = 1.0 - mean_psd_err / mean_psd_signal\n    level = [0.5]\n    cs = plt.contour(\n        1.0 / psd_based_score.freq_lon.values,\n        1.0 / psd_based_score.freq_time.values,\n        psd_based_score,\n        level,\n    )\n    x05, y05 = cs.collections[0].get_paths()[0].vertices.T\n    plt.close()\n\n    shortest_spatial_wavelength_resolved = np.min(x05)\n    shortest_temporal_wavelength_resolved = np.min(y05)\n    psd_da = 1.0 - mean_psd_err / mean_psd_signal\n    psd_da.name = \"psd_score\"\n    return (\n        psd_da.to_dataset(),\n        np.round(shortest_spatial_wavelength_resolved, 3).item(),\n        np.round(shortest_temporal_wavelength_resolved, 3).item(),\n    )\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.psd_based_scores_from_ds","title":"<code>psd_based_scores_from_ds(ds, ref_variable='tgt', study_variable='out')</code>","text":"<p>Compute PSD-based scores from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The dataset containing the reference and study variables.</p> required <code>ref_variable</code> <code>str</code> <p>The name of the reference variable.</p> <code>'tgt'</code> <code>study_variable</code> <code>str</code> <p>The name of the study variable.</p> <code>'out'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing PSD-based scores.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def psd_based_scores_from_ds(ds, ref_variable='tgt', study_variable='out'):\n    \"\"\"\n    Compute PSD-based scores from a dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the reference and study variables.\n        ref_variable (str): The name of the reference variable.\n        study_variable (str): The name of the study variable.\n\n    Returns:\n        list: A list containing PSD-based scores.\n    \"\"\"\n    try:\n        return psd_based_scores(ds[study_variable], ds[ref_variable])[1:]\n    except Exception:\n        return [np.nan, np.nan]\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.remove_nan","title":"<code>remove_nan(da)</code>","text":"<p>Fill NaN values in a DataArray using Gauss-Seidel interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <p>Returns:</p> Type Description <p>xarray.DataArray: The DataArray with NaN values filled.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def remove_nan(da):\n    \"\"\"\n    Fill NaN values in a DataArray using Gauss-Seidel interpolation.\n\n    Args:\n        da (xarray.DataArray): The input DataArray.\n\n    Returns:\n        xarray.DataArray: The DataArray with NaN values filled.\n    \"\"\"\n    da[\"lon\"] = da.lon.assign_attrs(units=\"degrees_east\")\n    da[\"lat\"] = da.lat.assign_attrs(units=\"degrees_north\")\n\n    da.transpose(\"lon\", \"lat\", \"time\")[:, :] = pyinterp.fill.gauss_seidel(\n        pyinterp.backends.xarray.Grid3D(da)\n    )[1]\n    return da\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.rmse_based_scores","title":"<code>rmse_based_scores(da_rec, da_ref)</code>","text":"<p>Compute RMSE-based scores for reconstruction evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>da_rec</code> <code>DataArray</code> <p>The reconstructed data.</p> required <code>da_ref</code> <code>DataArray</code> <p>The reference data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing RMSE-based scores.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def rmse_based_scores(da_rec, da_ref):\n    \"\"\"\n    Compute RMSE-based scores for reconstruction evaluation.\n\n    Args:\n        da_rec (xarray.DataArray): The reconstructed data.\n        da_ref (xarray.DataArray): The reference data.\n\n    Returns:\n        tuple: A tuple containing RMSE-based scores.\n    \"\"\"\n    rmse_t = (\n        1.0\n        - (((da_rec - da_ref) ** 2).mean(dim=(\"lon\", \"lat\"))) ** 0.5\n        / (((da_ref) ** 2).mean(dim=(\"lon\", \"lat\"))) ** 0.5\n    )\n    rmse_xy = (((da_rec - da_ref) ** 2).mean(dim=(\"time\"))) ** 0.5\n    rmse_t = rmse_t.rename(\"rmse_t\")\n    rmse_xy = rmse_xy.rename(\"rmse_xy\")\n    reconstruction_error_stability_metric = rmse_t.std().values\n    leaderboard_rmse = (\n        1.0 - (((da_rec - da_ref) ** 2).mean()) ** 0.5 / (((da_ref) ** 2).mean()) ** 0.5\n    )\n    return (\n        rmse_t,\n        rmse_xy,\n        np.round(leaderboard_rmse.values, 5).item(),\n        np.round(reconstruction_error_stability_metric, 5).item(),\n    )\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.rmse_based_scores_from_ds","title":"<code>rmse_based_scores_from_ds(ds, ref_variable='tgt', study_variable='out')</code>","text":"<p>Compute RMSE-based scores from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The dataset containing the reference and study variables.</p> required <code>ref_variable</code> <code>str</code> <p>The name of the reference variable.</p> <code>'tgt'</code> <code>study_variable</code> <code>str</code> <p>The name of the study variable.</p> <code>'out'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing RMSE-based scores.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def rmse_based_scores_from_ds(ds, ref_variable='tgt', study_variable='out'):\n    \"\"\"\n    Compute RMSE-based scores from a dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the reference and study variables.\n        ref_variable (str): The name of the reference variable.\n        study_variable (str): The name of the study variable.\n\n    Returns:\n        list: A list containing RMSE-based scores.\n    \"\"\"\n    try:\n        return rmse_based_scores(ds[study_variable], ds[ref_variable])[2:]\n    except Exception:\n        return [np.nan, np.nan]\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.test_osse","title":"<code>test_osse(trainer, lit_mod, osse_dm, osse_test_domain, ckpt, diag_data_dir=None)</code>","text":"<p>Perform OSSE (Observing System Simulation Experiment) testing and compute metrics.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer instance.</p> required <code>lit_mod</code> <code>LightningModule</code> <p>The Lightning module to test.</p> required <code>osse_dm</code> <code>LightningDataModule</code> <p>The datamodule for OSSE testing.</p> required <code>osse_test_domain</code> <code>dict</code> <p>The test domain for evaluation.</p> required <code>ckpt</code> <code>str</code> <p>Path to the checkpoint to load.</p> required <code>diag_data_dir</code> <code>Path</code> <p>Directory to save diagnostic data.</p> <code>None</code> <p>Returns:</p> Type Description <p>pandas.Series: A series containing OSSE metrics.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def test_osse(trainer, lit_mod, osse_dm, osse_test_domain, ckpt, diag_data_dir=None):\n    \"\"\"\n    Perform OSSE (Observing System Simulation Experiment) testing and compute metrics.\n\n    Args:\n        trainer (pl.Trainer): The PyTorch Lightning trainer instance.\n        lit_mod (pl.LightningModule): The Lightning module to test.\n        osse_dm (pl.LightningDataModule): The datamodule for OSSE testing.\n        osse_test_domain (dict): The test domain for evaluation.\n        ckpt (str): Path to the checkpoint to load.\n        diag_data_dir (Path, optional): Directory to save diagnostic data.\n\n    Returns:\n        pandas.Series: A series containing OSSE metrics.\n    \"\"\"\n    lit_mod.norm_stats = osse_dm.norm_stats()\n    trainer.test(lit_mod, datamodule=osse_dm, ckpt_path=ckpt)\n    osse_tdat = lit_mod.test_data[['out', 'ssh']]\n    osse_metrics = diagnostics_from_ds(\n        osse_tdat, test_domain=osse_test_domain\n    )\n\n    print(osse_metrics.to_markdown())\n\n    if diag_data_dir is not None:\n        osse_metrics.to_csv(diag_data_dir / \"osse_metrics.csv\")\n        if (diag_data_dir / \"osse_test_data.nc\").exists():\n            xr.open_dataset(diag_data_dir / \"osse_test_data.nc\").close()\n        osse_tdat.to_netcdf(diag_data_dir / \"osse_test_data.nc\")\n\n    return osse_metrics\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.triang_lr_adam","title":"<code>triang_lr_adam(lit_mod, lr_min=5e-05, lr_max=0.003, nsteps=200)</code>","text":"<p>Configure an Adam optimizer with triangular cyclic learning rate scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>lit_mod</code> <p>The Lightning module containing the model.</p> required <code>lr_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>5e-05</code> <code>lr_max</code> <code>float</code> <p>Maximum learning rate.</p> <code>0.003</code> <code>nsteps</code> <code>int</code> <p>Number of steps for the triangular cycle.</p> <code>200</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def triang_lr_adam(lit_mod, lr_min=5e-5, lr_max=3e-3, nsteps=200):\n    \"\"\"\n    Configure an Adam optimizer with triangular cyclic learning rate scheduling.\n\n    Args:\n        lit_mod: The Lightning module containing the model.\n        lr_min (float): Minimum learning rate.\n        lr_max (float): Maximum learning rate.\n        nsteps (int): Number of steps for the triangular cycle.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n    \"\"\"\n    opt = torch.optim.Adam(\n        [\n            {\"params\": lit_mod.solver.grad_mod.parameters(), \"lr\": lr_max},\n            {\"params\": lit_mod.solver.prior_cost.parameters(), \"lr\": lr_max / 2},\n        ],\n    )\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": torch.optim.lr_scheduler.CyclicLR(\n            opt,\n            base_lr=lr_min,\n            max_lr=lr_max,\n            step_size_up=nsteps,\n            step_size_down=nsteps,\n            gamma=0.95,\n            cycle_momentum=False,\n            mode=\"exp_range\",\n        ),\n    }\n</code></pre>"},{"location":"pkg-doc/utils/#ocean4dvarnet.utils.vort","title":"<code>vort(da)</code>","text":"<p>Compute the vorticity from a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <p>Returns:</p> Type Description <p>xarray.DataArray: The vorticity computed from the input data.</p> Source code in <code>ocean4dvarnet/utils.py</code> <pre><code>def vort(da):\n    \"\"\"\n    Compute the vorticity from a DataArray.\n\n    Args:\n        da (xarray.DataArray): The input DataArray.\n\n    Returns:\n        xarray.DataArray: The vorticity computed from the input data.\n    \"\"\"\n    return mpcalc.vorticity(\n        *mpcalc.geostrophic_wind(\n            da.pipe(add_geo_attrs).assign_attrs(units=\"m\").metpy.quantify()\n        )\n    ).metpy.dequantify()\n</code></pre>"},{"location":"usage/","title":"Index","text":""},{"location":"usage/#making-predictions-with-a-pre-trained-model","title":"Making predictions with a pre-trained model","text":"<p>If you would like to make predictions with a model that has already been trained, e.g. AIFS Single 1. See the Ocean4DVarNet-inference getting started guide.</p>"},{"location":"usage/#training-a-new-model-through-configuration","title":"Training a new model through configuration","text":"<p>If you would like to train a new model on a sample dataset, making use of building blocks which already exist in the Ocean4DVarNet codebase. See the Ocean4DVarNet-training getting started guide.</p>"},{"location":"usage/#contributing-use-cases","title":"Contributing use-cases","text":"<p>We would love to make Ocean4DVarNet even easier for everyone to use.</p> <p>If there are any use-cases you would like to see covered in the future, please consider contacting the developers or creating a pull request to the Ocean4DVarNet repository.</p>"},{"location":"usage/#sources","title":"Sources","text":"<ul> <li>https://anemoi.readthedocs.io/projects/training/en/latest/user-guide/overview.html#usage-getting-started</li> </ul>"}]}